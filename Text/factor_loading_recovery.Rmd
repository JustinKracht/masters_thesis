---
title             : "Factor Loading Recovery for Smoothed Non-positive Definite Correlation Matrices"
shorttitle        : "Factor Loading Recovery for Smoothed Matrices"

author: 
  - name          : "Justin D. Kracht"
    affiliation   : "1"
    corresponding : yes
    address       : "Department of Psychology, University of Minnesota, N218 Elliott Hall 75 East River Road, Minneapolis, MN 55455"
    email         : "krach018@umn.edu"

affiliation:
  - id            : "1"
    institution   : "University of Minnesota"
    
authornote:
appendix: 
  - "appendix.Rmd"

abstract: |
  Researchers commonly use tetrachoric correlation matrices in item factor analysis. Unfortunately, tetrachoric correlation matrices are often non-positive definite (i.e., having one or more negative eigenvalues). These indefinite correlation matrices are problematic because the corresponding population correlation matrices they estimate are definitionally positive semidefinite (PSD; i.e., having strictly non-negative eigenvalues). Therefore, when used in procedures such as factor analysis, indefinite tetrachoric correlation matrices may result in poor estimates of factor loadings. Matrix smoothing algorithms attempt to remedy this problem by finding a PSD correlation matrix that is close, in some sense, to a given indefinite correlation matrix. However, little research has been done on the effectiveness of matrix smoothing. In the present simulation study, indefinite tetrachoric correlation matrices were calculated from simulated binary data sets. Three matrix smoothing algorithms---the Higham (2002), Bentler-Yuan (2011), and Knol-Berger algorithms (1991)---were applied to the indefinite tetrachoric correlation matrices. Factor analysis was then conducted on the smoothed and unsmoothed correlation matrices. The results show that smoothed matrices were slightly better estimates of their population counterparts compared to unsmoothed indefinite correlation matrices. However, using smoothed compared to unsmoothed indefinite correlation matrices for item factor analysis did not meaningfully improve factor loading recovery. Matrix smoothing should therefore be considered only as a tool to facilitate factor analysis of indefinite correlation matrices and not as a statistical remedy for the root causes of matrix indefiniteness.
  
keywords          : "matrix smoothing, item factor analysis, factor loading recovery, non-positive definite"
wordcount         : "`r wordcountaddin::word_count(filename ='factor_loading_recovery.Rmd')`"

bibliography      : ["references.bib"]

floatsintext      : no
figurelist        : yes 
tablelist         : yes
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
nocite            : |
  @R-lme4, , @robustlmm, @box1964, @jacqmin2007, @butler1992, @verbeke1997, @zhang2001
  
documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
header-includes   : 
  - \usepackage{longtable, rotating}
  - \DeclareMathOperator{\tr}{tr}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)

# Round to three digits for inline code
inline_digits_hook <- function(x) {
  if (is.numeric(x)) {
    format(round(x, 3), digits = 3)
  } else x
}
knitr::knit_hooks$set(inline = inline_digits_hook)

pacman::p_load(knitr,
               ggplot2,
               tidyr,
               dplyr,
               mice,
               broom.mixed,
               here,
               lme4,
               latex2exp,
               texreg)

pacman::p_load_current_gh("crsh/papaja") # R markdown APA6 template and functions

# Set the project directory
project_dir <- here::here()
generate_figs <- FALSE # Do you want to regenerate all of the figures?
generate_tabs <- FALSE # Do you want to regenerate all of the tables?

# Load the data (only indefinite matrices)
results_matrix_npd <- readRDS(paste0(project_dir, 
                                     "/Data", 
                                     "/results_matrix_npd.RDS"))

# Make sure select and filter aren't masked
select <- dplyr::select
filter <- dplyr::filter
```

\newcommand{\Rsm}{\mathbf{R}_{\textrm{Sm}}}
\newcommand{\Rpop}{\mathbf{R}_{\textrm{Pop}}}
\newcommand{\Rnpd}{\mathbf{R}_{\textrm{Ind}}}
\newcommand{\Rapa}{\mathbf{R}_{\textrm{APA}}}
\newcommand{\Rby}{\mathbf{R}_{\textrm{BY}}}
\newcommand{\Rkb}{\mathbf{R}_{\textrm{KB}}}
\newcommand{\dg}{\textrm{dg}}
\newcommand{\RMSE}{\textrm{RMSE}(\mathbf{\Lambda}, \hat{\mathbf{\Lambda}})}

Tetrachoric correlation matrices [@olsson1979maximum] are used to estimate the correlations between the normally-distributed, continuous latent variables assumed to underlie observed binary data. Therefore, tetrachoric correlation matrices are often recommended for use in item factor analysis because the common linear factor model requires the assumption that outcomes are continuous [@wirth2007item]. Unfortunately, tetrachoric correlation matrices are frequently *indefinite*, having one or more negative eigenvalues [@wothke1993; @bock1988]. Indefinite correlation matrices are problematic because proper correlation matrices are, by definition, positive semi-definite [PSD; i.e., having all eigenvalues greater than or equal to 0; @wothke1993]. Although indefinite correlation matrices resemble proper correlation matrices in many ways---they are symmetric, have unit diagonals, and all off-diagonal elements $|r_{ij}| \leq 1$---it is impossible to obtain an indefinite matrix of Pearson correlations from complete data. Thus, indefinite correlation matrices are improper estimates of their corresponding population correlation matrices in the sense that they are not included in the set of possible population correlation matrices.

Some researchers have suggested that one approach to resolving the problem of indefinite tetrachoric correlation matrices is to obtain a PSD correlation matrix that can be reasonably substituted for an indefinite tetrachoric correlation matrix [e.g., @devlin1975robust; @dong1985non]. This approach is often referred to as *matrix smoothing* and many algorithms developed for this purpose, referred to as *matrix smoothing algorithms* (or simply, *smoothing algorithms*), have been proposed in the psychometric literature and elsewhere [@bentler2011;@fushiki2009;@higham2002;@knol1991;@li2010;@lurie1998;@qi2006;@devlin1975robust;@dong1985non]. However, despite the the frequent occurrence of indefinite tetrachoric correlation matrices in psychometric research [@bock1988, p. 261], the variety of smoothing algorithms available, and suggestions to use matrix smoothing algorithms as a remedy to indefinite tetrachoric correlation matrices [@wothke1993; @bentler2011; @knol1991], scant research has been done on the effectiveness of matrix smoothing algorithms in the context of item factor analysis of indefinite tetrachoric correlation matrices. In one of the only published comparisons of this kind, @knol1991 investigated the effects of using smoothed compared to unsmoothed correlation matrices in factor analysis and found no large differences. However, this comparison was not a main focus of their study and only compared a small number of indefinite matrices (10 indefinite correlation matrices with 250 subjects and 15 items). 

Moreover, few studies have compared the relative performance of matrix smoothing algorithms in the context of factor analysis [@debelak2013; @debelak2016]. Debelak and Tran (2013) conducted a simulation study to determine which of three matrix smoothing algorithms --- the Higham Alternating Projections algorithm (APA; 2002), Bentler-Yuan algorithm (BY; 2011), and the Knol-Berger (KB; 1991) algorithm --- most often recovered the underlying dimensionality when applied to indefinite tetrachoric correlation matrices prior to parallel analysis [@horn1965]. Debelak and Tran simulated binary data using a two-parameter logistic (2PL) item response theory [IRT; @birnbaum1968some; @de2013theory] model for one- and two-factor models with varying factor correlations, item difficulties, item discriminations, numbers of items, and numbers of subjects. Debelak and Tran then computed tetrachoric correlation matrices for each simulated binary data set. If a tetrachoric correlation matrix was indefinite, the three aforementioned smoothing algorithms were applied (resulting in three smoothed correlation matrices in addition to the indefinite tetrachoric matrix). Finally, Debelak and Tran conducted parallel analysis using each of these four correlation matrices to obtain estimates of dimensionality. Debelak and Tran concluded that "[the] application of smoothing algorithms generally improved correct identification of dimensionality when the correlation between the latent dimensions was 0.0 or 0.4 in our simulations" [@debelak2013, p. 74]. With respect to the relative performance of the Higham, Bentler-Yuan, and Knol-Berger smoothing algorithms in this context, Debelak and Tran concluded that there were "minor differences in the performance of the three smoothing algorithms used in [the] study. In data sets with a clear dimensional structure...the algorithm of Bentler and Yuan (2011) performed best" [@debelak2013, p. 74].

Following on these results, Debelak and Tran (2016) extended their simulation study design to evaluate the relative and absolute effectiveness of matrix smoothing algorithms when applied to indefinite polychoric correlation matrices of ordered, categorical (i.e., polytomous) data prior to conducting  a parallel analysis. As in their previous study, Debelak and Tran used the accuracy of the parallel analysis dimensionality estimates (i.e., dimensionality recovery) as their evaluation criterion. In addition to extending their design to consider polytomous data, Debelak and Tran (2016) also considered factor models with either one or three major common factors and either zero or forty minor common factors. The minor common factors were meant to represent the effects of model approximation error; that is, the degree of model misfit inherent to mathematical models of natural phenomena in general, and psychological models in particular [@tucker1969; @maccallum1991representing; @maccallum2001]. Debelak and Tran concluded that the analysis of smoothed polychoric correlation matrices generally gave more accurate results than the analysis of indefinite polychoric correlation matrices. Moreover, they found that "methods based on the algorithms of Knol and Berger, Higham, and Bentler and Yuan showed a comparable performance with regard to the accuracy to detect the number of underlying major factors, with a slightly better performance of methods based on the Bentler and Yuan algorithm" [@debelak2016, p. 15].

Both Debelak and Tran (2013) and Debelak and Tran (2016) concluded that the Bentler-Yuan (2011) smoothing algorithm led to the most accurate results (in terms of dimesionality recovery) when applied to indefinite tetrachoric or polychoric correlation matrices. However, neither study attempted to explain why the Bentler-Yuan algorithm led to better dimensionality recovery relative to the other smoothing methods they investigated. One intriguing possibility is that the smoothed correlation matrices produced by the Bentler-Yuan algorithm were better approximations of population correlation matrix than either the smoothed matrices produced by the Knol-Berger (1991) and Higham algorithms (2002), and also better approximations than the original indefinite tetrachoric or polychoric correlation matrices. If this is true, one might also expect that Bentler-Yuan smoothed tetrachoric correlation matrices will also lead to more accurate factor loading estimates compared to the alternatives. 

The purpose of the present study was to address two questions related to these hypotheses. First, are smoothed indefinite tetrachoric correlation matrices better estimates of their corresponding population correlation matrices than the original indefinite tetrachoric correlation matrices and, if so, which smoothing method produces the best estimates? Second, do smoothed indefinite tetrachoric correlation matrices lead to better factor loading estimates compared to the unsmoothed tetrachoric matrices when used in exploratory factor analysis and, if so, which smoothing algorithm leads to the best factor loading estimates? To answer these questions, I conducted a simulation study in which I generated `r prettyNum(length(unique(results_matrix_npd$id)), big.mark = ",")` indefinite tetrachoric correlation matrices from a variety of realistic data scenarios. Before describing the simulation design, I first introduce the three matrix smoothing algorithms under investigation, the common factor model, and the three factor analysis algorithms included in this study.

## Matrix Smoothing Algorithms

### Higham Alternating Projections Algorithm (APA; 2002)

The matrix smoothing algorithm proposed by Higham (2002) seeks to find the closest PSD correlation matrix to a given indefinite correlation matrix. In this context, closeness is defined as the generalized Euclidean distance [@banerjee2014, p. 492]. Higham's algorithm uses a series of alternating projections to locate the PSD correlation matrix ($\Rapa$) closest to a given indefinite correlation matrix ($\Rnpd$) of the same order. The algorithm works by first projecting $\Rnpd$ onto the set of symmetric, PSD $p \times p$ matrices, $\mathcal{S}$. The resulting candidate matrix is then projected onto the set of symmetric $p \times p$ matrices with unit diagonals, $\mathcal{U}$. The series of projections repeats until the algorithm converges to a matrix, $\Rapa$, that is PSD, symmetric, and has a unit diagonal, or until the maximum number of iterations is exceeded.

### Bentler-Yuan Algorithm (BY; 2011)

Bentler and Yuan (2011) proposed a smoothing algorithm based on minimum-trace factor analysis [MTFA; @bentler1972; @jamshidian1998]. MTFA seeks to find optimal communality estimates such that unexplained common variance is minimized while constraining the diagonal matrix of unique variances and the observed covariance matrix with the estimated communalities as diagonal elements to be positive semidefinite (PSD). In contrast with the Higham algorithm (2002), the Bentler-Yuan algorithm does not seek to minimize some criterion. Instead, the algorithm uses MTFA to identify Heywood cases [i.e., communality estimates greater than or equal to one and, consequently, negative or zero uniqueness variance estimates; @dillon1987]. The Bentler-Yuan algorithm then rescales the rows and columns of $\Rnpd$ corresponding to these Heywood cases to produce a smoothed, PSD correlation matrix, $\Rby$. More specifically, the algorithm first conducts an MTFA using $\Rnpd$. Using the results of the MTFA, a diagonal matrix, $\mathbf{H}$ is constructed containing the estimated communalities as diagonal elements. Next, another diagonal matrix, $\mathbf{\Delta}^2$, is constructed with elements $\delta_i^2$ where $\delta_i^2 = 1$ if $h_i < 1$ and $\delta_i^2 = k / h_i$ otherwise (where $k < 1$ is some constant). Finally, the smoothed, PSD correlation matrix $\Rby = \mathbf{\Delta} \mathbf{R}_0 \mathbf{\Delta} + \mathbf{I}$ is obtained, where $\mathbf{R}_0$ is $\Rnpd$ with diagonal elements replaced by zeroes and $\mathbf{I}$ is an identity matrix that ensures that $\Rby$ has a unit diagonal.

Similar to the Higham algorithm, the Bentler-Yuan algorithm sometimes fails to produce a PSD correlation matrix. This can happen either when (a) the MTFA algorithm fails to converge or (b) when $k$ is too large and does not shrink the targeted elements of the indefinite correlation matrix enough for the matrix to become PSD. To help with this non-convergence, I modified the Bentler-Yuan algorithm to adaptively select an appropriate $k$ by initializing at $k = 0.999$ and decreasing $k$ by $0.001$ until the algorithm produced a PSD correlation matrix or $k = 0$.[^k-value]

[^k-value]: Bentler and Yuan suggest using $k = 0.96$ [@bentler2011, p. 120] but claim that the precise value of $k$ does not matter a great deal as long as $k$ is marginally less than one.

### Knol-Berger Algorithm (KB; 1991)

In contrast to the Higham (2002) and Bentler-Yuan (2011) smoothing algorithms, the Knol-Berger algorithm is a non-iterative procedure in which the negative eigenvalues of $\Rnpd$ are replaced with some small positive value. The first step in the Knol-Berger algorithm is to compute the eigendecomposition of the $p \times p$ indefinite correlation matrix, $\Rnpd = \mathbf{V \Lambda V}^\prime$, where $\mathbf{V}$ is an orthonormal matrix containing the eigenvectors of $\Rnpd$ and $\mathbf{\Lambda}$ is a diagonal matrix with the eigenvalues of $\Rnpd$, $\lambda_i, \: i \in \{1, \dots, p \}$, ordered from largest to smallest on the diagonal ($\lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_p$, $\lambda_p < 0$). Next, a matrix $\mathbf{\Lambda_+}$ is created by setting all negative elements of $\mathbf{\Lambda}$ equal to some user-specified small, positive constant. Finally, a smoothed, PSD correlation matrix, $\Rkb$, is constructed by replacing $\mathbf{\Lambda}$ with $\mathbf{\Lambda_+}$ in the eigendecomposition of $\Rnpd$ and then scaling to ensure a unit diagonal and all off-diagonal elements $|r_{ij}| \leq 1$:

\begin{equation}
\Rkb = [\dg(\mathbf{V \Lambda_+ V}^\prime)]^{-1/2} \mathbf{V \Lambda_+ V}^\prime \dg(\mathbf{V \Lambda_+ V}^\prime)]^{-1/2},
(\#eq:Rkb)
\end{equation}

where $\dg(\mathbf{V \Lambda_+ V}^\prime)$ returns a diagonal matrix containing the diagonal elements of $\mathbf{V \Lambda_+ V}^\prime$ [@magnus2019matrix, p. 440].

## The Common Factor Model

The linear factor analysis model is used to describe the variance of each observed variable in terms of the contributions of a small number of latent common factors and a specific factor unique to that variable [@wirth2007item]. In the common factor model, the population correlation matrix, $\mathbf{P}$, can be expressed as:

\begin{equation}
\mathbf{P} = \mathbf{F} \mathbf{\Phi} \mathbf{F}^{\prime} + \mathbf{\Theta}^2
(\#eq:cfa)
\end{equation}

where $\mathbf{P}$ is a $p \times p$ population correlation matrix for $p$ observed variables, $\mathbf{F}$ is a $p \times m$ factor loading matrix for $m$ common factors, $\mathbf{\Phi}$ is an $m \times m$ matrix of correlations between the $m$ common factors, and $\mathbf{\Theta}^2$ is a $p \times p$ diagonal matrix containing the unique variances.

Although the common factor analysis model is often useful, many authors have remarked that it constitutes an oversimplification of the complex processes that generate real, observed data [@cudeck1991model; @maccallum1991representing; @maccallum2001]. Tucker, Koopman, and Linn [-@tucker1969] suggested that the lack-of-fit between the common factor model and the complex processes underlying real data could be accounted for by modeling a large number of minor, common factors. The effects of these minor common factors represent model approximation error. The model Tucker et al. (1969) proposed can be written as:
\begin{equation}
\mathbf{P} = \mathbf{F} \mathbf{\Phi} \mathbf{F}^{\prime} + \mathbf{\Theta}^2 + \mathbf{WW}^{\prime}
(\#eq:cfa-error)
\end{equation}
where $\mathbf{W}$ is a $p \times q$ matrix containing factor loadings for the $q \gg m$ minor factors [@briggs2003, p. 32]. Given our expectation that the common factor model is not a perfect representation of any real-world data-generating process we might wish to represent, \@ref(eq:cfa-error) should be preferred to \@ref(eq:cfa) for simulating realistic data [@hong1999; @briggs2003].

## Factor Analysis Algorithms
<!-- c -->
One purpose of this study was to determine whether the effects of matrix smoothing method on factor loading recovery differ depending on the factor analysis algorithm used. To that end, three factor analysis methods were used in the current simulation: principal axes (PA), ordinary least-squares (OLS), and maximum-likelihood (ML). These factor analysis methods were chosen because they are some of the most commonly used methods [@fabrigar1999evaluating] and because two of the methods (PA and OLS) work when an indefinite correlation matrix is given as input.

### Principal Axes Factor Analysis
<!-- c -->
Principal axes (PA) factor analysis is conceptually similar to principal components analysis (PCA). Whereas PCA seeks to find a low-dimensional approximation of the full observed correlation matrix, PA seeks to find a low-dimensional approximation of the reduced correlation matrix, $\mathbf{R}_*$ (i.e., the observed correlation matrix, $\mathbf{R}$, with communalities on the diagonal). Because the true communalities are unknown, principal axes factor analysis starts by using estimated communalities to form $\mathbf{R}_*$.[^1] The eigenvalues of $\mathbf{R}_*$ are then taken to be the updated communality estimates. These updated estimates replace the previous estimates on the diagonal of $\textbf{R}_*$ and the procedure iterates until the sum of the differences between the communality estimates from the current and previous iterations is less than some small convergence criterion.

[^1]: Many methods of estimating communalities have been proposed, the most common of which are the squared multiple correlation between each variable and the other variables [@mulaik2009foundations, p. 182; @roff1936some; @dwyer1939contribution] and the maximum absolute correlation between each variable and the other variables [@mulaik2009foundations, p. 175; @thurstone1947multiple]. However, the particular choice of initial communality estimates has been shown to not have a large effect on the final solution when the convergence criterion is sufficiently stringent [@widaman1985iterative].

### Ordinary Least-Squares Factor Analysis
<!-- c -->
The ordinary least-squares factor analysis method [OLS; also known as "minres"; @comrey1962minimum] seeks to minimize the sum of squared differences between the sample correlation matrix, $\mathbf{R}$, and $\hat{\mathbf{P}}$, the correlation matrix implied by the estimated factor model defined in \@ref(eq:cfa). The OLS discrepancy function can then be written as
\begin{equation}
F_{OLS}(\mathbf{R}, \hat{\mathbf{P}}) = \frac{1}{2} \tr \left[ (\mathbf{R} - \hat{\mathbf{P}})^2 \right],
(\#eq:ls-discrepancy)
\end{equation}
where $\tr$ is the trace operator [@magnus2019matrix, p. 11] and $\tr \left[ (\mathbf{R} - \hat{\mathbf{P}})^2 \right]$ is the trace (sum of the diagonal elements) of the matrix formed by $(\mathbf{R} - \hat{\mathbf{P}})^2$. OLS does not give additional weight to residuals corresponding to large correlations and requires no assumptions about the population distributions of the variables [@briggs2003].

### Maximum-Likelihood Factor Analysis
<!-- c -->
The maximum likelihood factor analysis algorithm (ML) is similar to OLS in that it seeks to minimize the discrepancy between $\mathbf{R}$ and $\hat{\mathbf{P}}$. Unlike OLS, however, ML assumes that all variables are multivariate normal in the population. Then, we can write the discrepancy function to be minimized as an alternative form of the multivariate normal log-likelihood function,
\begin{equation}
F_{ML}(\mathbf{R}, \hat{\mathbf{P}}) = \log|\hat{\mathbf{P}}| - \log|\mathbf{R}| + \tr(\mathbf{S}\hat{\mathbf{P}}^{-1}) - p.
(\#eq:ml-discrepancy)
\end{equation}
In addition to the distributional assumptions required by ML factor analysis, the method also assumes that the only source of error in the model is sampling error. Consequently, large correlations (having relatively small standard errors) are fit more closely than small correlations (with relatively large standard errors) under maximum likelihood factor analysis [@briggs2003]. Also note that when $\mathbf{R}$ is indefinite, $|\mathbf{R}|$ is negative and $\log |\mathbf{R}|$ is undefined. Therefore, indefinite covariance or correlation matrices cannot be used as input for maximum likelihood factor analysis.

# Simulation Procedure
<!-- c -->
I conducted a simulation study to evaluate four approaches to dealing with indefinite tetrachoric correlation matrices (applying matrix smoothing using the Higham [2002], Bentler-Yuan [2011], or Knol-Berger [1991] algorithms, or leaving indefinite tetrachoric matrices unsmoothed) in the context of exploratory factor analysis. The simulation study was designed to address two primary questions. First, which smoothing method (Higham, Bentler-Yuan, Knol-Berger, or None) produced (possibly) smoothed correlation matrices ($\Rsm$) that most closely approximated the corresponding population correlation matrices? Second, which smoothing method produced correlation matrices that led to the best estimates of the population factor loading matrix when used in exploratory factor analyses?

In the first step of the simulation study, I generated random sets of binary data from a variety of orthogonal factor models with varying numbers of major common factors ($\textrm{Factors} \in \{1, 3, 5, 10 \}$). Following the procedure of @tucker1969, I also incorporated the effects of model approximation error into the data by including 150 minor common factors in each population model. In total, these 150 minor common factors accounted for 0%, 10%, or 30% ($\textrm{Error} \in \{ 0, .1, .3 \}$) of the uniqueness variance of the error-free model (i.e., the model with only the major common factors). These conditions were chosen to represent models with perfect, good, or moderate model fit, resembling the conditions used by Briggs and MacCallum [-@briggs2003]. These three levels of model approximation error in the simulation ensured that both ideal ($\textrm{Error} = 0$) and more empirically-plausible levels of model approximation error ($\textrm{Error} \in \{ .1, .3\}$) were considered in this study.

In addition to systematically varying the number of major factors and the proportion of variance accounted for by model approximation error, I also varied the number of factor indicators (i.e., items loading on each factor; $\textrm{Items/Factor} \in \{5, 10 \}$), and the number of subjects per item ($\textrm{Subjects/Item} \in \{ 5, 10, 15\}$). The total numbers of items and sample sizes for each factor number condition can be found in Table \@ref(tab:items-subjects-table). Each item loaded on only one factor and item factor loadings were uniformly fixed at one of three levels ($\textrm{Loading} \in \{ .3, .5, .8 \}$). Though "rules-of-thumb" for factor loadings vary, Hair, Black, Babin, and Anderson [-@hair2018, p. 151] suggest that "[f]actor loadings in the range of $\pm 0.30$ to $\pm 0.40$ are considered to meet the minimal level for interpretation of structure", and "[l]oadings $\pm 0.50$ or greater are considered practically significant." Moreover, factor loadings of $\pm 0.8$ are considered to be high [@maccallum2001]. Thus, the three factor loadings investigated in this study were chosen to represent low, moderate, and high levels of factor salience.
<!-- c -->
(ref:items-subjects-table-caption) Number of items and subjects resulting from each combination of number of factors (Factors), number of items per factor (Items/Factor), and subjects per item (Subjects/Item).

The combinations of the independent variables specified above resulted in a fully-crossed design with $4 \: (\textrm{Factors}) \times 3 \: (\textrm{Error}) \times 2 \: (\textrm{Items/Factor}) \times 3 \: (\textrm{Subjects/Item}) \times 3 \: (\textrm{Loading}) = 216$ unique conditions. For each of these conditions, I used the `simFA` function in the R [Version 3.6.2; @R-base][^r-packages] *fungible* library [Version 1.95.4.8; @R-fungible] to generate 1,000 random sets of data in accordance with the factor model corresponding to that condition. To obtain binary responses from continuous observed scores, items were assigned classical item difficulties [$d$; i.e., the expected proportion of correct responses, @CrockerAlgina] at equal intervals between 0.15 and 0.85. For example, items in a five-item data set were assigned classical item difficulties of .150, .325, .500, .675, and .850. The classical item difficulties were used to obtain threshold values, $t$, such that $P(X > t) = d$ where $X \sim N(0,1)$. I then used the thresholds to dichotomize the continuous observed scores and obtain simulated binary response data. If a data set had any homogeneous item response vectors (i.e., had one or more items with zero variance), the data set was discarded and a new sample of data was generated until all items had non-homogeneous response vectors. This procedure was necessary to calculate tetrachoric correlation matrices in the next step of the simulation.

[^r-packages]: Additionally, I used the following R packages: *arm* [Version 1.10.1; @R-arm], *broom.mixed* [Version 0.2.4; @R-broom.mixed], *car* [Version 3.0.7; @R-car], *dplyr* [Version 0.8.5; @R-dplyr], *forcats* [Version 0.5.0; @R-forcats], *ggplot2* [Version 3.3.0; @R-ggplot2], *here* [Version 0.1.11; @R-here], *knitr* [Version 1.28; @R-knitr], *koRpus* [Version 0.11.5; @R-koRpus; @R-koRpus.lang.en], *koRpus.lang.en* [Version 0.1.3; @R-koRpus.lang.en], *latex2exp* [Version 0.4.0; @R-latex2exp], *lattice* [Version 0.20.38; @R-lattice], *lme4* [Version 1.1.23; @R-lme4], *MASS* [Version 7.3.51.4; @R-MASS], *Matrix* [Version 1.2.18; @R-Matrix], *merTools* [Version 0.5.0; @R-merTools], *papaja* [Version 0.1.0.9942; @R-papaja], *patchwork* [Version 1.0.0; @R-patchwork], *purrr* [Version 0.3.4; @R-purrr], *questionr* [Version 0.7.0; @R-questionr], *readr* [Version 1.3.1; @R-readr], *sfsmisc* [Version 1.1.4; @R-sfsmisc], *stringr* [Version 1.4.0; @R-stringr], *sylly* [Version 0.1.5; @R-sylly], *texreg* [Version 1.36.23; @R-texreg], *tibble* [Version 3.0.1; @R-tibble], *tidyr* [Version 1.0.2.9000; @R-tidyr], *tidyverse* [Version 1.3.0; @R-tidyverse], *viridis* [Version 0.5.1; @R-viridis], and *wordcountaddin* [Version 0.3.0.9000; @R-wordcountaddin].

Next, I calculated a tetrachoric correlation matrix for each simulated binary data set. Tetrachoric correlation matrices were calculated using the `tetcor` function in the R *fungible* library [@R-fungible], which computes maximum likelihood tetrachoric correlation coefficients [@olsson1979maximum; @brown1977mean]. If a tetrachoric correlation matrix was indefinite, the Higham (2002), Bentler-Yuan (2011), and Knol-Berger (1991) matrix smoothing algorithms were applied to the indefinite tetrachoric correlation matrix to produce three smoothed, PSD correlation matrices. Matrix smoothing was done using the `smoothAPA`, `smoothBY`, and `smoothKB` implementations of the Higham (2002), Bentler-Yuan (2011), and Knol-Berger (1991) algorithms in *fungible*.

In the third and final step of the simulation procedure, I applied three exploratory factor analysis algorithms (principal axes [PA], ordinary least squares [OLS], and maximum likelihood [ML]) to each of the indefinite tetrachoric correlation matrices and the PSD, smoothed correlation matrices. Because ML does not work with indefinite correlation or covariance matrices as input, ML was conducted on the Pearson correlation matrix (rather than the indefinite tetrachoric correlation matrix) when no smoothing was applied. Each of the factor solutions were then rotated using a quartimin rotation [@jennrich2002; @carroll1957] and aligned to match the corresponding population factor loading matrix such that the least squares discrepancy between the matrices was minimized. The alignment step ensured that the elements of each estimated factor loading matrix were matched (in order and sign) to the elements of the corresponding population factor loading matrix. These rotation and alignment steps were accomplished using the `faAlign` and `faMain` functions in the R *fungible* library [@R-fungible]. Code for all aspects of this study including the simulation, analyses, figures, and tables is available at https://github.umn.edu/krach018/masters_thesis.
<!-- c -->
```{r items-subjects-table, echo = FALSE, results = "asis"}
# Create a table giving the total numbers of items and subjects in each factor
# number condition
x <- expand.grid("Factors" = c(1, 3, 5, 10, 15),
                 "Items/Factor" = c(5, 10),
                 "Subjects/Item" = c(5, 10, 15))
x$'Items' <- x$Factors * x$`Items/Factor`
x$'Sample Size' <- x$Items * x$`Subjects/Item`

knitr::kable(
  x = x, 
  caption = "(ref:items-subjects-table-caption)",
  digits = 0,
  longtable = TRUE,
  booktabs = TRUE
)
```

# Results

## Recovery of the population correlation matrix

One of the primary reasons for conducting the present simulation study was to determine which of the three investigated smoothing methods --- the Higham (2002), Bentler-Yuan (2011), or Knol-Berger (1991) algorithms --- resulted in smoothed correlation matrices that were closest to the correlation matrix implied by the major factor model (i.e., the factor model not including the minor factors). In particular, I examined whether smoothed correlation matrices were closer to the model-implied correlation matrix than the unsmoothed, indefinite correlation matrix. In this context, the scaled distance between two $p \times p$ correlation matrices $\mathbf{A} = \{a_{ij}\}$ and $\mathbf{B} = \{ b_{ij} \}$ was computed as:

\begin{equation}
\mathrm{D}_{\mathrm{s}}(\mathbf{A}, \mathbf{B})=\sqrt{\sum_{i=1}^{p-1} \sum_{j=i+1}^{p} \frac{\left(a_{i j}-b_{i j}\right)^{2}}{p(p-1)/2}}.
(\#eq:scaled-distance)
\end{equation}

```{r RpopRsm-summary, echo = FALSE}
# Select only unique smoothed matrix observations
RpopRsm_data <- results_matrix_npd %>%
  select(id:npd, smoothing_method, distance_Rpop_Rsm, 
         factors_rec:model_error_rec) %>%
  distinct()

# Calculate number of npd correlation matrices
npd_matrices <- RpopRsm_data %>%
  select(id, smoothing_method) %>%
  filter(smoothing_method == "KB") %>%
  nrow()

# Convergence rates for the smoothing algorithms
smoothing_convergence_tab <- RpopRsm_data %>%
  group_by(smoothing_method) %>%
  summarise(n_obs = n(),
            n_converged = sum(!is.na(distance_Rpop_Rsm), na.rm = TRUE),
            prop_converged = mean(!is.na(distance_Rpop_Rsm), na.rm = TRUE))

# Mean D(Rpop, Rsm) by smoothing method
# For incomplete data
RpopRsm_summary_tab <- RpopRsm_data %>%
  group_by(smoothing_method) %>%
  summarise(n_obs = n(),
            RpopRsm_mean = mean(distance_Rpop_Rsm, na.rm = TRUE),
            RpopRsm_sd = sd(distance_Rpop_Rsm, na.rm = TRUE))

# Tidy model summary
RpopRsm_mod <- readRDS(file = here("Data", "RpopRsm_model.RDS"))
Ds_mod <- broom::tidy(RpopRsm_mod)
remove(RpopRsm_mod)
```

To understand which of the smoothing algorithms most often produced a smoothed correlation matrix, $\Rsm$, that was closest to the model-implied correlation matrix, $\Rpop$, I calculated $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$ for each $\Rsm$ obtained from the `r formatC(npd_matrices, big.mark = ",")` indefinite tetrahoric correlation matrices. Small values of $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$ indicated that the smoothed correlation matrix was a good approximation of $\Rpop$, whereas large values indicated that $\Rsm$ was a poor approximation of $\Rpop$. After excluding the three cases where the Higham (2002) algorithm failed to converge, I calculated the mean $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$ for each of the remaining `r formatC(nrow(RpopRsm_data), big.mark = ",")` smoothed matrices. On average, the Bentler-Yuan algorithm produced smoothed correlation matrices that were slightly closer to $\Rpop$ (*M* = `r RpopRsm_summary_tab$RpopRsm_mean[RpopRsm_summary_tab$smoothing_method == "BY"]`, *SD* = `r RpopRsm_summary_tab$RpopRsm_sd[RpopRsm_summary_tab$smoothing_method == "BY"]`) than the smoothed matrices produced by the Knol-Berger (*M* = `r RpopRsm_summary_tab$RpopRsm_mean[RpopRsm_summary_tab$smoothing_method == "KB"]`, *SD* = `r RpopRsm_summary_tab$RpopRsm_sd[RpopRsm_summary_tab$smoothing_method == "KB"]`) or Higham (*M* = `r RpopRsm_summary_tab$RpopRsm_mean[RpopRsm_summary_tab$smoothing_method == "APA"]`, *SD* = `r RpopRsm_summary_tab$RpopRsm_sd[RpopRsm_summary_tab$smoothing_method == "APA"]`) algorithms. The mean distance between the indefinite correlation matrices, $\Rnpd$, and $\Rpop$ was larger (*M* = `r RpopRsm_summary_tab$RpopRsm_mean[RpopRsm_summary_tab$smoothing_method == "None"]`, *SD* = `r RpopRsm_summary_tab$RpopRsm_sd[RpopRsm_summary_tab$smoothing_method == "None"]`) than the mean distances for any of the three smoothing algorithms.

To get a more detailed look at the how the smoothed correlation matrices approximated $\Rpop$, I fit a linear mixed-effects model regressing $\log \mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$ on number of subjects per item (5, 10, 15), number of items per factor (5, 10), number of factors (1, 3, 5, 10), factor loading (0.3, 0.5, 0.8), model error (0.0, 0.1, 0.3), smoothing algorithm (Higham, Bentler-Yuan, Knol-Berger, or no smoothing), all two-way interactions between these variables, and a random intercept estimated for every unique indefinite correlation matrix.[^2] The estimated fixed-effect coefficients are in Figure \@ref(fig:coefplot-RpopRsm). A full summary table for the model appears in Table \@ref(tab:distance-mod-summary).

[^2]: All numeric predictors were scaled to have a mean of zero and variance of one prior to analysis. Diagnostic plots can be found in Appendix A.

Figure \@ref(fig:coefplot-RpopRsm) shows that only a few variables had non-trivial effects on population matrix recovery. In particular, the three most potent effects were number of factors ($b = -0.313$, SE = 0.0004, $e^{-.313} = 0.731$), number of subjects per item ($b = -0.221$, SE = 0.0005, $e^{-0.221} = 0.802$), and number of items per factor ($b = -0.164$, SE = 0.0004, $e^{-0.164} = 0.849$). These estimated effects were all negative, indicating better recovery of the population correlation matrix for models with larger numbers of major factors, larger numbers of subjects per item, and larger numbers of items per factor, all else being equal. All three smoothing algorithms were associated with small (negative) estimated main effects. In particular, the estimated main effect for the Bentler-Yuan algorithm was smallest ($b = -0.073$, SE = 0.0001, $e^{-0.073} = 0.930$), followed by the estimated main effects for the Knol-Berger ($b = -0.033$, SE = 0.0001, $e^{-0.033} = 0.968$) and Higham ($b = -0.024$, SE = 0.0001, $e^{-0.024} = 0.976$) algorithms. These main effects were offset somewhat by small, positive interaction effects between the smoothing methods and subjects per item, and between the smoothing methods and factor loading (see Table \@ref(tab:distance-mod-summary) and Figure \@ref(fig:coefplot-RpopRsm)).

To get a better sense of the relative performance of the smoothing algorithms (in terms of population correlation matrix recovery), Figure \@ref(fig:distance-Rpop-Rsm) shows box-plots of $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$ for all combinations of smoothing method, factor loading size, number of subjects per item, and number of items per factor. The most apparent feature of Figure \@ref(fig:distance-Rpop-Rsm) was the improvement of population correlation matrix recovery as number of items per factor and number of subjects per item increased. The conditions with loadings fixed at 0.3, and 15 subjects per item (see the cells in the upper right-hand corner of Figure \@ref(fig:distance-Rpop-Rsm)) might seem to go against the trend of lower $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$ for higher numbers of subjects per item. However, these results are likely an artifact of the lack of indefinite correlation matrices for these combinations of conditions (only `r results_matrix_npd %>% filter(factor_loading == 0.3 & subjects_per_item == 15 & items_per_factor == 5) %>% select(id) %>% unique() %>% nrow()` and `r results_matrix_npd %>% filter(factor_loading == 0.3 & subjects_per_item == 15 & items_per_factor == 10) %>% select(id) %>% unique() %>% nrow()` indefinite tetrachoric correlation matrices for the five items-per-factor and ten items-per-factor conditions, respectively). The other important trend in Figure \@ref(fig:distance-Rpop-Rsm) was that the Bentler-Yuan algorithm performed best relative to the other smoothing methods in conditions with few subjects per item and items per factor. However, this advantage became nearly imperceptible as the numbers of subjects per item and items per factor increased. The interaction between the Bentler-Yuan algorithm and magnitude of factor loading was also evident, such that the Bentler-Yuan algorithm performed worse as factor loadings increased.

Taken as a whole, the results suggest that three variables accounted for the majority of the variation in $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$: (a) the number of major factors in the data-generating model, (b) the number of subjects per item, and (c) the number of items per (major) factor. Increases in any of these variables were associated with improved population correlation matrix recovery. Choice of smoothing method was also related to population correlation matrix recovery, to some extent. In particular, smoothed matrices were slightly closer to the population correlation matrix than the unsmoothed tetrachoric correlation matrices. Of the three smoothing algorithms used, the Bentler-Yuan algorithm produced smoothed matrices that were closest to the population correlation matrices. However, differences between smoothing methods were small except in conditions with few subjects per item, few items per factor, and low factor loadings.
<!-- c -->
```{r coefplot-RpopRsm, echo = FALSE, fig.cap = 'Exponentiated coefficient estimates for the linear mixed effects model using $\\log[\\mathrm{D}_{\\mathrm{s}}(\\Rsm, \\Rpop)]$ as the dependent variable and estimating a random intercept for each indefinite correlation matrix. The Higham (2002), Bentler-Yuan (2011) and Knol-Berger (1991) algorithms are denoted as APA, BY, and KB, respectively. The effect of the condition where no smoothing was applied is subsumed within the Constant term.', out.width='100%', fig.align="center"}
if (generate_figs) {
  source("R/figure_code/coef_plots.R")
}

knitr::include_graphics(path = paste0(project_dir, 
                                      "/Text/figs/RpopRsm_coefplot.png"),
                        dpi = 320)
```
<!-- c -->
```{r distance-Rpop-Rsm, fig.cap = "Scaled distance between the smoothed ($\\Rsm$) and model-implied ($\\Rpop$) correlation matrices for the Higham (APA; 2002), Bentler-Yuan (BY; 2011), and Knol-Berger (KB; 1991) smoothing methods and when no smoothing was applied (None).", eval = TRUE, echo = FALSE, out.width='100%', fig.align="center"}
# fig.env = "sidewaysfigure" for sideways figure, if needed
if (generate_figs) {
  source("R/figure_code/distance_Rpop_Rsm.R")
}

knitr::include_graphics(path = paste0(project_dir, 
                                      "/Text/figs/distance_Rpop_Rsm.png"),
                        dpi = 320)
```
<!-- c -->
```{r distance-mod-summary, eval = TRUE, results = 'asis', echo = FALSE}
if (generate_figs) {
  source("R/table_code/coef_tab_Rpop_Rsm.R")
}

cat(readLines(paste0(project_dir, "/Text", "/tabs", "/RpopRsm_coef_tab.txt")),
    sep = "\n")
```
<!-- c -->
## Recovery of factor loadings
<!-- c -->
I next analyzed the results in terms of factor loading recovery. In particular, I sought to determine whether factor analysis of smoothed matrices led to better factor loading estimates than unsmoothed matrices, and if particular smoothing methods led to better factor loading estimates than others. I was also interested in whether the interactions between smoothing methods and the other variables (e.g., number of items per factor, number of subjects per item, factor analysis method, etc.) affected the relative smoothing algorithm performance in terms of factor loading estimation. For the purposes of these analyses, I evaluated factor loading recovery using the root-mean-square error (RMSE) between the estimated and population factor loadings for the major factors. Given a matrix of estimated major factor loadings $\hat{\mathbf{\Lambda}} = \{ \hat{\lambda}_{ij} \}_{p \times m}$, and the corresponding matrix of population major factor loadings, $\mathbf{\Lambda} = \{ \lambda_{ij} \}_{p \times m}$,
<!-- c -->
\begin{equation}
\textrm{RMSE}(\mathbf{\Lambda}, \hat{\mathbf{\Lambda}}) = \sqrt{\sum_{i = 1}^{p} \sum_{j = 1}^{m} \frac{(\lambda_{ij} - \hat{\lambda}_{ij})^2}{pm}}.
(\#eq:rmse)
\end{equation}
<!-- c -->
```{r calculate-convergence-and-rmse, echo = FALSE}
# Convergence rates for the factor analysis algorithms
fa_convergence_tab <- results_matrix_npd %>%
  filter(!is.na(distance_Rpop_Rsm)) %>%
  group_by(fa_method_rec) %>%
  summarise(n_obs = n(),
            n_converged = sum(fa_convergence, na.rm = TRUE),
            prop_converged = mean(fa_convergence, na.rm = TRUE))

# Mean RMSE values by smoothing method and factor analysis method
# For incomplete data
loading_summary_tab <- results_matrix_npd %>%
  filter(!is.na(distance_Rpop_Rsm)) %>%
  group_by(smoothing_method) %>%
  summarise(n_obs = n(),
            mean_rmse = mean(loading_rmsd, na.rm = TRUE),
            sd_rmse = sd(loading_rmsd, na.rm = TRUE))

# Save the number of PA non-converged matrices and number of remaining cases as
# objects that can be easily referenced in the text.
pa_nonconverged <- fa_convergence_tab$n_obs[fa_convergence_tab$fa_method_rec == "PA"] -
  fa_convergence_tab$n_converged[fa_convergence_tab$fa_method_rec == "PA"]
pa_convergence_rate <- fa_convergence_tab$prop_converged[fa_convergence_tab$fa_method_rec == "PA"] * 100
n_complete_obs <- results_matrix_npd %>%
  filter(!is.na(loading_rmsd) & fa_convergence != FALSE) %>%
  nrow()
```
<!-- c -->
To determine which smoothing method resulted in the best factor loading estimates, I calculated the $\RMSE$ for each pair of estimated and population factor loading matrices corresponding to the (possibly) smoothed indefinite tetrachoric correlation matrices. Relatively small $\RMSE$ values indicated that the estimated factor loading matrices were more similar to their corresponding population factor loading matrices, whereas larger $\RMSE$ values indicated poorly-estimated factor loading matrices. As in the previous section, the four cases where the Higham (2002) algorithm did not converge were not included in my analyses. Furthermore, cases where PA failed to converge were also not included. In total, there were `r prettyNum(pa_nonconverged, big.mark = ",")` cases where the PA algorithm did not converge (convergence rate = `r pa_convergence_rate`%) and only four cases where the ML algorithm did not converge (convergence rate > 99.9%). For the `r prettyNum(n_complete_obs, big.mark = ",")` cases remaining, factor analysis of the Bentler-Yuan (2011) smoothed matrices resulted in the lowest mean $\RMSE$ (*M* = `r loading_summary_tab$mean_rmse[loading_summary_tab$smoothing_method == "BY"]`, *SD* = `r loading_summary_tab$sd_rmse[loading_summary_tab$smoothing_method == "BY"]`) whereas the smoothed matrices produced by the Higham (2002; *M* = `r loading_summary_tab$mean_rmse[loading_summary_tab$smoothing_method == "APA"]`, *SD* = `r loading_summary_tab$sd_rmse[loading_summary_tab$smoothing_method == "APA"]`) and Knol-Berger (1991; *M* = `r loading_summary_tab$mean_rmse[loading_summary_tab$smoothing_method == "KB"]`, *SD* = `r loading_summary_tab$sd_rmse[loading_summary_tab$smoothing_method == "KB"]`) algorithms led to slightly higher mean $\RMSE$ values. Factor analyzing unsmoothed indefinite tetrachoric correlation matrices led to the highest mean $\RMSE$ (*M* = `r loading_summary_tab$mean_rmse[loading_summary_tab$smoothing_method == "None"]`, *SD* = `r loading_summary_tab$sd_rmse[loading_summary_tab$smoothing_method == "None"]`).

To obtain estimates of effects, I fit a linear mixed-effects model regressing $\log \RMSE$ on number of subjects per item, number of items per factor, number of factors, factor loading, model error, smoothing algorithm (Higham, Bentler-Yuan, Knol-Berger, or no smoothing), factor analysis method (PA, OLS, or ML), all two-way interactions between these variables, and a random intercept estimated for every unique indefinite correlation matrix.[^3] The estimated (exponentiated) fixed-effect coefficients are shown in Figure \@ref(fig:coefplot-loading-recovery). A full summary table including (untransformed) coefficient estimates and standard errors appears in Table \@ref(tab:loading-mod-summary).

[^3]: All numeric predictors were scaled to have a mean of zero and variance of one prior to analysis. Diagnostic plots can be found in Appendix A.

Figure \@ref(fig:coefplot-loading-recovery) shows that only a few variables had non-negligible effects on factor loading recovery. None of the effects of primary interest to this study---the main effects or two-way interactions involving the smoothing methods---were large enough to hold much practical significance. For instance, the results indicated that not applying smoothing to an indefinite tetrachoric correlation matrix prior to factor analysis led to the worst factor loading estimates among the four smoothing methods. However, this effect represented only a minute improvement in $\RMSE$ for smoothed compared to unsmoothed indefinite tetrachoric correlation matrices (see Figure \@ref(fig:coefplot-loading-recovery) and Table \@ref(tab:loading-mod-summary)).

Although none of the primary effects of interest to this study were large, there were some estimated effects that, although ancillary for this study, were large enough to warrant mention. In particular, there were moderate, positive effects for ML ($b = 0.116$, SE = 0.0004, $e^{0.116} = 1.123$) and the interactions between ML and factor loading ($b = 0.204$, SE = 0.0002, $e^{0.204} = 1.226$), ML and subjects per item ($b = 0.107$, SE = 0.0002, $e^{0.107} = 1.113$), and ML and items per factor ($b = 0.082$, SE = 0.0002, $e^{0.082} = 1.085$). There was also a positive estimated effect for model error ($b = 0.104$, SE = 0.0005, $e^{0.104} = 1.11$). These results suggest that both the use of ML factor analysis (compared with PA or OLS) and higher levels of model approximation error led to worse factor loading recovery. Moreover, factor loading estimates for ML were not improved as much by increasing factor loadings, subjects per item, or items per factor (which were associated with negative effects) compared to PA or OLS. To illustrate this interaction, the effects of factor analysis method, factor loading, and number of subject per item are shown in Figure \@ref(fig:fa-method-boxplots), which contains box plots of $\RMSE$ for each combination of these variables. This figure shows that ML often led to the lowest $\RMSE$ values in conditions with small factor loadings but did not improve as much as OLS or PA as factor loadings increased. Similar effects can be seen for the number of subjects per item; although $\RMSE$ values generally decreased as number of subjects per item increased for all factor analysis methods, ML seemed to benefit least.[^4] These results should be interpreted carefully, however, because ML factor analysis when no smoothing was applied was (by necessity) conducted on Pearson correlation matrices whereas the other factor analysis algorithms were applied to the indefinite tetrachoric correlation matrices in the no smoothing condition.

[^4]: Number of items per factor was not included in Figure \@ref(fig:fa-method-boxplots) because the differences in $\RMSE$ across levels of the condition were too small to be clearly seen.

There were also large, negative effects for the variables that should be expected to related to improved factor loading recovery, namely, factor loading ($b = -0.438$, SE = 0.0006, $exp^{-0.438} = 0.645$), subjects per item ($b = -0.189$, SE = 0.0007, $exp^{-0.189} = 0.828$), and items per factor ($b = -0.175$, SE = 0.0005, $exp^{-0.175} = 0.839$). There was also a relatively large and seemingly anomalous negative effect for number of factors ($b = -0.255$, SE = 0.0005, $exp^{-0.255} = 0.775$). On its face, this effect seems to suggest that data generated from models with large numbers of major factors led to better factor loading recovery. However, this effect is most likely due to the fact that, whereas number of items per factor and number of subjects per item were fully-crossed with (orthogonal to) number of factors, the total sample size and number of items for each data set were confounded with number of factors. In other words, conditions with larger numbers of factors tended to include more items in total, and therefore also tended to have larger sample sizes despite having the same numbers of indicators (items) per factor and numbers of subjects per indicator. The strong relationship between $\log \RMSE$ and total sample size can be clearly seen in Figure \@ref(fig:RMSE-sample-size), which shows that $\log \RMSE$ decreased as sample size increased. Therefore, it seems reasonable to conclude that the effect of number of factors can be better understood as being related to the total number of items and subjects in a data set. Similarly, the negative interaction between number of factors and ML ($b = -0.090$, SE = 0.0002, $exp^{-0.090} = 0.914$) might be interpreted as an interaction between total number of items or subjects and ML. In summary, the results of the simulation study indicated that there was no meaningful advantage of using any smoothing algorithm over any other. Moreover, there was no large advantage (in terms of $\RMSE$) to smoothing indefinite tetrachoric correlation matrices prior to conducting exploratory factor analysis.
<!-- The interaction agrees with a result from DeWinter & Dodou (2012, p. 708) that showed that ML gets better as sample size increases when there is model misspecification in the form of underfactoring, compared with PA. -->
```{r fa-method-boxplots, fig.cap="Box plots of $\\RMSE$ for all combinations of factor analysis method, factor loading, and number of subjects per item. The three factor analysis methods (ordinary least squares, maximum likelihood, and principal axes) are denoted by OLS, ML, and PA, respectively.", fig.align="center"}
if (generate_figs) {
  source("R/figure_code/fa_method_boxplots.R")
}

knitr::include_graphics(path = paste0(project_dir, 
                                      "/Text/figs/fa_method_boxplots.png"),
                        dpi = 320)
```
<!-- c -->
```{r RMSE-sample-size, fig.cap="Log root-mean-square error (RMSE) between the true and estimated factor loading matrices as a function of sample size. Due to the large number of data points, hexagonal bins were used to group observations with the density of each hexagon represented by its color.", fig.align="center"}
if (generate_figs) {
  source("R/figure_code/rmse_sample_size.R")
}

knitr::include_graphics(path = paste0(project_dir, 
                                      "/Text/figs/rmse_sample_size.png"),
                        dpi = 320)
```
<!-- c -->
```{r coefplot-loading-recovery, eval = TRUE, echo = FALSE, fig.cap = "Exponentiated coefficient estimates for the linear mixed effects model using $\\log[\\RMSE]$ as the dependent variable and estimating a random intercept for each indefinite correlation matrix. The Higham (2002), Bentler-Yuan (2011) and Knol-Berger (1991) algorithms are denoted as APA, BY, and KB, respectively. Maximum likelihood factor analysis is denoted by ML and principal axis factor analysis is denoted by PA. The effects of no smoothing and ordinary least squares factor analysis are subsumed within the Constant term.", out.width='100%', fig.align="center"}
knitr::include_graphics(path = paste0(project_dir, 
                                      "/Text/figs/loadings_coefplot.png"),
                        dpi = 320)
```
<!-- c -->
```{r loading-mod-summary, eval = TRUE, results = 'asis', echo = FALSE}
if (generate_tabs) {
  source("R/table_code/coef_tab_loadings.R")
}

cat(readLines(paste0(project_dir, "/Text", "/tabs", "/loading_coef_tab.txt")),
    sep = '\n')
```
<!-- c -->
# Discussion

## Interpretation of Results

The current study examined how the application of three matrix smoothing algorithms (the Higham [2002], Bentler-Yuan [2011], and Knol-Berger [1991] algorithms) to indefinite tetrachoric correlation matrices affected both (a) the recovery of the model-implied population correlation matrix ($\Rpop$), and (b) the recovery of the population item factor loadings in EFA (compared to leaving the indefinite correlation matrices unsmoothed). With respect to recovery of $\Rpop$, I found that the application of any of the matrix smoothing algorithms included in the present study led to slightly better recovery of the $\Rpop$ compared to the unsmoothed, indefinite tetrachoric correlation matrix. Of the three matrix smoothing algorithms included in this study, the application of the Bentler-Yuan algorithm (2011) produced the best approximations of $\Rpop$ (on average). In particular, the Bentler-Yuan algorithm led to the best results relative to the other smoothing algorithms in conditions with low factor loadings, few items per factor, and few subjects per item. However, differences between smoothing algorithms (in terms of recovery of $\Rpop$) were mostly so small as to be of little practical importance. With respect to the recovery of population factor loadings, I found that the particular matrix smoothing algorithm applied to an indefinite tetrachoric correlation matrix prior to EFA led to no meaningful differences in factor loading recovery. Moreover, conducting EFA on smoothed, PSD correlation matrices led to only marginally better factor loading recovery compared to conducting EFA on indefinite, unsmoothed correlation matrices.

## Limitations and Future Directions

As with any simulation study, the present simulation design was not able to cover the full range of realistic data scenarios. For instance, the simulation design included only orthogonal population factor models and did not allow for correlated factors. Future research on this topic should investigate whether more complex correlation structures affect the performance of matrix smoothing algorithms in terms of population correlation matrix recovery and factor loading recovery. Moreover, the present studies only investigated the effects of matrix smoothing on indefinite tetrachoric correlation matrices. Further research should be done to investigate the effects of matrix smoothing on indefinite polychoric correlation matrices, as well as correlation matrices that are indefinite due to other causes. For instance, indefinite correlation matrices calculated using pairwise deletion [@wothke1993] or composite correlation matrices used in meta-analysis [@furlow2005]. Little is known about whether the mechanism or "cause" of indefinite correlation matrices affects their structure or how these potential differences might interact with the application of matrix smoothing algorithms. 

Future research should also investigate ways to side-step the problem of indefinite tetrachoric correlation matrices. For instance, Choi, Kim, Chen, and Dannels [-@choi2011] found that polychoric correlation matrices estimated using expected a posteriori (EAP) rather than maximum-likelihood estimation led to estimates that were negatively biased but produced comparable (or smaller) RMSE values in terms of recovering the "true" correlations. It seems plausible that the slight shrinkage induced by using EAP as an estimation method would make indefinite tetrachoric or polychoric correlation matrices less common. Finally, full-information maximum likelihood [FIML; @bock1981] can be used to estimate model parameters directly and doesn't require the estimation of a tetrachoric correlation matrix. Future research should investigate whether the use of FIML (which is computationally intenstive, particularly with large models) offers any benefit, in terms of parameter recovery, when applied to datasets corresponding to indefinite tetrachoric correlation matrices.

## Conclusion

Despite the lackluster improvement in factor loading recovery when factor analysis was conducted on smoothed rather than indefinite tetrachoric correlation matrices, the application of one of the three investigated matrix smoothing algorithms on indefinite tetrachoric correlation matrices is still recommended. None of the smoothing algorithms regularly led to worse results (in terms of factor loading recovery) compared to the conditions where the indefinite correlation matrix was left unsmoothed. Moreover, all of the smoothing algorithms investigated in this study are computationally inexpensive and are readily available as functions in R packages. For instance, the *fungible* [@R-fungible], *sfsmisc* [@R-sfsmisc], and *Matrix* [@R-Matrix] packages all contain implementations of at least one of the three smoothing algorithms discussed in this article. In particular, the Knol-Berger algorithm (1991) is recommended as a smoothing algorithm that is fast, easily implemented in most programming languages, does not have convergence issues, and led to results comparable to the Bentler-Yuan and Higham algorithms. 

This recommendation comes with a strong caveat; Namely, that no matrix smoothing algorithm can reasonably be considered a remedy or solution for indefinite tetrachoric correlation matrices. Instead, researchers should consider indefinite tetrachoric correlation matrices to be symptoms of larger problems (e.g., small sample sizes, bad items. etc.) and be aware that practical solutions such as gathering more data or discarding bad items are likely to lead to better results than the application of matrix smoothing algorithms. In particular, indefinite tetrachoric correlation matrices are less likely to occur when sample sizes are large relative to the number of items [see Table 1 in @debelak2013, p. 70], allowing researchers to avoid the question of how to properly deal with an indefinite tetrachoric correlation matrix entirely. If collecting more data is not possible, researchers should consider removing problematic items. In short, all three investigated smoothing algorithms are reasonable choices for dealing with indefinite tetrachoric correlation matrices prior to factor analysis and seem to offer a modest benefit (in terms of factor loading recovery) compared to leaving the indefinite tetrachoric correlation matrix unsmoothed. However, the application of these algorithms should be considered to be little more than a band-aid fix that does not address the underlying issues leading to indefinite tetrachoric correlation matrices.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
