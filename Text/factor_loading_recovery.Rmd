---
title             : "Factor Loading Recovery for Smoothed Non-positive Definite Correlation Matrices"
shorttitle        : "Factor Loading Recovery for Smoothed Matrices"

author: 
  - name          : "Justin D. Kracht"
    affiliation   : "1"
    corresponding : yes
    address       : "Department of Psychology, University of Minnesota, N218 Elliott Hall 75 East River Road, Minneapolis, MN 55455"
    email         : "krach018@umn.edu"

affiliation:
  - id            : "1"
    institution   : "University of Minnesota"

authornote: |
  Enter author note here.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "`r wordcountaddin::word_count(filename ='factor_loading_recovery.Rmd')`"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
header-includes   : 
  - \usepackage{longtable, rotating}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)

# Round to three digits for inline code
inline_digits_hook <- function(x) {
  if (is.numeric(x)) {
    format(round(x, 3), digits = 3)
  } else x
}
knitr::knit_hooks$set(inline = inline_digits_hook)

pacman::p_load(knitr,
               ggplot2,
               tidyr,
               dplyr,
               mice,
               broom.mixed,
               here,
               lme4,
               latex2exp,
               texreg)

pacman::p_load_gh("crsh/papaja") # R markdown APA6 template and functions

# Set the project directory
project_dir <- "/Volumes/GoogleDrive/My Drive/School/masters_thesis"
generate_figs <- FALSE # Do you want to regenerate all of the figures?
generate_tabs <- FALSE # Do you want to regenerate all of the tables?

# Load the data (only NPD matrices)
results_matrix_npd <- readRDS(paste0(project_dir, 
                                     "/Data", 
                                     "/results_matrix_npd.RDS"))
```

\newcommand{\Rsm}{\mathbf{R}_{\textrm{Sm}}}
\newcommand{\Rpop}{\mathbf{R}_{\textrm{Pop}}}
\newcommand{\Rnpd}{\mathbf{R}_{\textrm{NPD}}}
\newcommand{\Rapa}{\mathbf{R}_{\textrm{APA}}}
\newcommand{\Rby}{\mathbf{R}_{\textrm{BY}}}
\newcommand{\Rkb}{\mathbf{R}_{\textrm{KB}}}
\newcommand{\dg}{\textrm{dg}}
\newcommand{\RMSE}{\textrm{RMSE}(\mathbf{\Lambda}, \hat{\mathbf{\Lambda}})}

Tetrachoric correlation matrices [@olsson1979maximum] are often recommended for use in item factor analysis [@wirth2007item]. However, tetrachoric correlation matrices are frequently *non-positive definite* (NPD), having one or more negative eigenvalues.

## Matrix Smoothing Algorithms

One approach to resolving the problem of NPD tetrachoric correlation matrices is to obtain a PSD correlation matrix that can be reasonably substituted for an NPD tetrachroic correlation matrix. The process obtaining a PSD correlation matrix to substitute for an NPD correlation matrix is often referred to as *matrix smoothing*. Many algorithms, referred to as *matrix smoothing algorithms* (or simply, *smoothing algorithms*), have been proposed in the psychometric literature and elsewhere [@bentler2011;@fushiki2009;@higham2002;@knol1991;@li2010;@lurie1998;@qi2006;@devlin1975robust;@dong1985non]. However, despite the variety of smoothing algorithms available, the frequent occurrence of NPD tetrachoric correlation matrices in psychometric research [@bock1988, p. 261], and suggestions to use matrix smoothing algorithms as a remedy to NPD tetrachoric correlation matrices [@wothke1993; @bentler2011; @knol1991], few studies have evaluated the effectiveness of matrix smoothing algorithms in the context of item factor analysis of NPD tetrachoric correlation matrices (**citations needed**). Moreover, only two studies have compared the relative performance of matrix smoothing algorithms in the context of factor analysis [@debelak2013; @debelak2016].

In the first of these two comparisons of matrix smoothing algorithms, Debelak and Tran (2013) conducted a simulation study to determine which of three matrix smoothing algorithms --- the Higham Alternating Projections algorithm (APA; 2002), Bentler-Yuan algorithm (BY; 2011), and the Knol-Berger (KB; 1991) algorithm --- led to the highest rates of underlying dimensionality recovery when applied to NPD tetrachoric correlation matrices prior to parallel analysis [@horn1965]. Debelak and Tran simulated binary data using a two-parameter logistic (2PL) item response theory [IRT; @birnbaum1968some; @de2013theory] for one- and two-factor models with varying factor correlations, item difficulties, item discriminations, numbers of items, and numbers of subjects. Debelak and Tran then computed tetrachoric correlation matrices for each simulated binary data set. If a tetrachoric correlation matrix was NPD, the three aforementioned smoothing algorithms were applied (resulting in three smoothed correlation matrices in addition to the NPD tetrachoric matrix). Finally, Debelak and Tran conducted parallel analyses four sets of correlation matrices: (a) the set of PSD tetrachoric correlation matrices and unsmoothed NPD correlation matrices; (b) the set of PSD tetrachoric correlation matrices and smoothed matrices produced by Higham algorithm (2002); (c) the set of PSD tetrachoric correlation matrices and smoothed matrices produced by the Bentler-Yuan algorithm (2011); and (d), the set of PSD tetrachoric correlation matrices and smoothed matrices produced by the Knol-Berger algorithm (1991). With respect to the effectiveness of matrix smoothing in the context of the application of parallel analysis to tetrachoric correlation matrices, Debelak and Tran concluded that "[the] application of smoothing algorithms generally improved correct identification of dimensionality when the correlation between the latent dimensions was 0.0 or 0.4 in our simulations" [@debelak2013, p. 74]. With respect to the relative performance of the Higham, Bentler-Yuan, and Knol-Berger smoothing algorithms in this context, Debelak and Tran concluded that there were "minor differences in the performance of the three smoothing algorithms used in [the] study. In data sets with a clear dimensional structure ... the algorithm of Bentler and Yuan (2011) performed best" [@debelak2013, p. 74].

Following on these results, Debelak and Tran (2016) extended their simulation study design to evaluate the relative and absolute effectiveness of matrix smoothing algorithms when applied to NPD polychoric correlation matrices of ordered, categorical (i.e., polytomous) data prior to conducting  a parallel analysis. As in their previous study, Debelak and Tran used the accuracy of the parallel analysis dimensionality estimates (i.e., dimensionality recovery) as their evaluation criterion. In addition to extending their design to consider polytomous data, Debelak and Tran (2016) also differed from the design of their previous study by considering factor models with either one or three major common factors and either zero or forty minor common factors. The minor common factors were meant to represent the effects of model approximation error; that is, the degree of model misfit inherent to mathematical models of natural phenomena in general, and psychological models in particular [@tucker1969; @maccallum1991representing; @maccallum2001]. Debelak and Tran concluded that the analysis of smoothed polychoric correlation matrices generally gave more accurate results than the analysis of NPD polychoric correlation matrices. Moreover, they found that "methods based on the algorithms of Knol and Berger, Higham, and Bentler and Yuan showed a comparable performance with regard to the accuracy to detect the number of underlying major factors, with a slightly better performance of methods based on the Bentler and Yuan algorithm" [@debelak2016, p. 15].

Both Debelak and Tran (2013) and Debelak and Tran (2016) conclude that the Bentler-Yuan (2011) smoothing algorithm leads to the most accurate results when applied to NPD tetrachoric or polychoric correlation matrices in the context of parallel analysis. However, neither study attempted to explain why the Bentler-Yuan algorithm led to more accurate dimensionality recovery in the context of parallel analysis. One intriguing possibility is that the smoothed correlation matrices produced by the Bentler-Yuan algorithm are better approximations of population correlation matrix than either the smoothed matrices produced by the Knol-Berger and Higham algorithms, and better approximations than the original NPD tetrachoric or polychoric correlation matrices. If this is true, we might also expect that Bentler-Yuan smoothed tetrachoric correlation matrices will also lead to more accurate results compared to the alternatives (unsmoothed, NPD tetrachoric correlation matrices or matrices smoothed using the Higham or Knol-Berger algorithms) when used to obtain factor loading estimates in other analysis procedures that require sample correlation matrices (many exploratory factor anlaysis procedures, for instance). The aim of this study was to answer two main questions related to these hypotheses. First, are smoothed NPD tetrachoric correlation matrices better estimates of their corresponding population correlation matrices than the original NPD tetrachoric correlation matrices? If so, which smoothing method produces the best estimates? Second, do smoothed NPD tetrachoric correlation matrices lead to better factor loading estimates compared to the unsmoothed tetrachoric matrices when used in exploratory factor analysis? If so, which smoothing algorithm leads to the best factor loading estimates?

### Higham Alternating Projections Algorithm (APA; 2002)

The matrix smoothing algorithm proposed by Higham (2002) seeks to find the closest PSD correlation matrix to the provided NPD correlation matrix. In this context, closeness is defined as the generalized Euclidean distance [@banerjee2014, p. 492]. Higham's algorithm uses a series of alternating projections to locate the PSD correlation matrix ($\Rapa$) closest to a given  NPD correlation matrix ($\Rnpd$) of the same order. The algorithm works by first projecting $\Rnpd$ onto the set of symmetric, PSD $p \times p$ matrices, $\mathcal{S}$. The resulting candidate matrix is then projected onto the set of symmetric $p \times p$ matrices with unit diagonals, $\mathcal{U}$. The series of projections repeats until the algorithm converges to a matrix that PSD, symmetric, and has a unit diagonal, or until the maximum number of iterations is exceeded.

### Bentler-Yuan Algorithm (BY; 2011)

Bentler and Yuan (2011) proposed a smoothing algorithm based on minimum-trace factor analysis [MTFA; @bentler1972; @jamshidian1998]. In contrast with the Higham algorithm (2002), the Bentler-Yuan algorithm does not seek to minimize some criterion. Instead, the algorithm uses first uses MTFA to identify Heywood cases [i.e., communality estimates greater than or equal to one and, consequently, negative or zero uniqueness variance estimates; @dillon1987]. The Bentler-Yuan algorithm then rescales the rows and columns of $\Rnpd$ corresponding to these Heywood cases to produce a smoothed, PSD correlation matrix, $\Rby$. More specifically, the algorithm procedure is as follows. First, conduct an MTFA using the NPD correlation matrix, $\Rnpd$. Using the results of the MTFA, construct a diagonal matrix, $\mathbf{H}$ containing the estimated communalities as diagonal elements. Next, construct another diagonal matrix, $\mathbf{\Delta}^2$, with elements $\delta_i^2$ where $\delta_i^2 = 1$ if $h_i < 1$ and $\delta_i^2 = k / h_i$ otherwise (where $k < 1$ is some constant). Finally, construct the smoothed, PSD correlation matrix $\Rby = \mathbf{\Delta} \mathbf{R}_0 \mathbf{\Delta} + \mathbf{I}$ where $\mathbf{R}_0$ is $\Rnpd$ with diagonal elements replaced by zeroes and $\mathbf{I}$ is an identity matrix that ensures that $\Rby$ has a unit diagonal.

Similar to the Higham algorithm, the Bentler-Yuan algorithm sometimes fails to produce a PSD correlation matrix. This can happen either when (a) the MTFA algorithm fails to converge or (b) when $k$ is too large and does not shrink the targeted elements of the NPD correlation matrix enough for the matrix to become PSD. To help with this non-convergence, I modified the Bentler-Yuan algorithm to adaptively select an appropriate $k$ by initializing at $k = 0.999$ and decreasing $k$ by $0.001$ until the algorithm produced a PSD correlation matrix.

### Knol-Berger Algorithm (KB; 1991)

In contrast with the Higham (2002) and Bentler-Yuan (2011) smoothing algorithms, the Knol-Berger algorithm is a non-iterative procedure in which the negative eigenvalues of the NPD tetrachoric correlation matrix are replaced with some small positive value. The first step in the Knol-Berger is to compute the eigendecomposition of the $p \times p$ NPD correlation matrix, $\Rnpd = \mathbf{V \Lambda V}^\prime$, where $\mathbf{V}$ is an orthonormal matrix containing the eigenvectors of $\Rnpd$ and $\mathbf{\Lambda}$ is a diagonal matrix with the eigenvalues of $\Rnpd$, $\boldsymbol{\lambda}$, ordered from greatest to least on the diagonal ($\lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_p$, $\lambda_p < 0$). Next, a matrix $\mathbf{\Lambda_+}$ is created by setting all negative elements of $\mathbf{\Lambda}$ equal to some user-specified small, positive constant. Finally, a smoothed, PSD correlation matrix, $\Rkb$, is constructed by replacing $\mathbf{\Lambda}$ with $\mathbf{\Lambda_+}$ in the eigendecomposition of $\Rnpd$ and then scaling to ensure a unit diagonal and all off-diagonal elements $|r_{ij}| \leq 1$:

\begin{equation}
\Rkb = [\dg(\mathbf{V \Lambda_+ V}^\prime)]^{-1/2} \mathbf{V \Lambda_+ V}^\prime \dg(\mathbf{V \Lambda_+ V}^\prime)]^{-1/2},
(\#eq:Rkb)
\end{equation}

where the $\dg(\mathbf{V \Lambda_+ V}^\prime)$ returns a diagonal matrix containing the diagonal elements of $\mathbf{V \Lambda_+ V}^\prime$ [@magnus2019matrix, p. 440].

## The Common Factor Model

The linear factor analysis model is often used to describe the variance of each observed variable in terms of the contributions of a small number of latent common factors and a specific factor unique to that variable [@wirth2007item]. In the common fator model, the population correlation matrix, $\mathbf{\tilde{P}}$, can be expressed as:

\begin{equation}
\mathbf{\tilde{P}} = \mathbf{F} \mathbf{\Phi} \mathbf{F}^{\prime} + \mathbf{\Theta}^2
(\#eq:cfa)
\end{equation}

where $\mathbf{\tilde{P}}$ is a $p \times p$ population correlation matrix for $p$ observed variables, $\mathbf{F}$ is a $p \times m$ factor loading matrix for $m$ common factors, $\mathbf{\Phi}$ is an $m \times m$ matrix of correlations between the $m$ common factors, and $\mathbf{\Theta}^2$ is a $p \times p$ diagonal matrix containing the unique variances.

Although the common factor analysis model is often useful, many authors have remarked that it constitutes an oversimplification of the complex processes that generate real, observed data [@cudeck1991model; @maccallum1991representing; @maccallum2001]. Tucker, Koopman, and Linn [-@tucker1969] suggested that the lack-of-fit between the common factor model and the complex processes underlying real data could be better accounted for by modeling a large number of minor, common factors. The effects of these minor common factors represent model error. The model they proposed can be written as:

\begin{equation}
\mathbf{\tilde{P}} = \mathbf{F} \mathbf{\Phi} \mathbf{F}^{\prime} + \mathbf{\Theta}^2 + \mathbf{WW}^{\prime}
(\#eq:cfa-error)
\end{equation}

where $\mathbf{W}$ is a $p \times q$ matrix containing factor loadings for the $q \gg m$ minor factors [@briggs2003, p. 32].

**COME BACK AND REVISE** One purpose of my study was to evaluate three of the most common [@fabrigar1999evaluating] factor extraction methods: principal axes (PA), unweighted least-squares (ULS), and maximum-likelihood (ML). For background, brief descriptions of these methods and their rationales are given below.

### Principal Axes Factor Analysis

Principal axes (PA) factor extraction is a factor extraction procedure that has much in common with principal components analysis (PCA). Whereas PCA seeks to find low-dimensional approximation of the full observed correlation matrix, PA seeks to find a low-dimensional approxiation of the reduced correlation matrix (i.e., the observed correlation matrix with communalities on the diagonal). Because the true communalities are unknown, principal axes factor extraction starts by inserting estimated communalities into the diagonal of the observed correlation matrix, $\mathbf{R}$ to form the reduced correlation matrix, $\mathbf{R}_*$.[^1] The eigenvalues of $\mathbf{R}_*$ are then taken to be the updated communality estimates. These updated estimates are replace the previous estimates on the diagonal of $\textbf{R}_*$ and the procedure iterates until the sum of the differences between the communality estimates is less than some small convergence criterion.

[^1]: Many methods of estimating communalities have been proposed, the most common of which are the squared multiple correlation between each variable and the other variables [@mulaik2009foundations, p. 182; @roff1936some; @dwyer1939contribution] and the maximum absolute correlation between each variable and the other variables [@mulaik2009foundations, p. 175; @thurstone1947multiple]. However, the particular choice of initial communality estimates has been shown to not have a large effect the final solution when the convergence criterion is sufficiently stringent [@widaman1985iterative].

### Unweighted Least-Squares Factor Analysis

**MINIMIZE THE RESIDUAL ERROR BETWEEN R and $\mathbf{\Sigma}$**

### Maximum-Likelihood Factor Analysis

# Methods

I designed and ran a simulation study to evaluate four approaches to dealing with NPD tetrachoric correlation matrices in the context of exploratory factor analysis. Namely, I conducted exploratory factor analyses on tetrachoric correlation matrices smoothed using the Higham (2002), the Bentler-Yuan (2011), Knol-Berger (1991) algorithms and on unsmoothed (NPD and PSD) tetrachoric correlation matrices. I designed the simulation study with two primary purposes in mind. First, I wanted to know which smoothing method (Higham, Bentler-Yuan, Knol-Berger, or None) produced (possibly) smoothed correlation matrices ($\Rsm$) that most closely approximated the corresponding population correlation matrices. Second, I wanted to know which smoothing method produced correlation matrices that led to the best estimates of the population factor loading matrix when used in exploratory factor analyses. With these two purposes in mind, I conducted our simulation study as follows.

In the first step of the simulation, I generated random sets of binary data randomly generated from a variety of orthogonal factor models. The factor models had varying numbers of major common factors, $\textrm{Factors} \in \{1, 3, 5, 10 \}$. Following the procedure of [@tucker1969], I also incorporated the effects of model approximation error into the data by including 150 minor common factors in each population model. In total, these 150 minor common factors accounted for 0%, 10%, or 30% ($\textrm{Error} \in \{ 0, .1, .3 \}$) of the uniqueness variance of the error-free model (i.e., the model with only the major common factors). According to Briggs and MacCallum [-@briggs2003], these conditions represent models with perfect, good, or moderate model fit. Including these three levels of model approximation error in the simulation ensured that both ideal ($\textrm{Error} = 0$) and more empirically-plausible levels of model approximation error ($\textrm{Error} \in \{ .1, .3\}$) were considered in this study.

In addition to systematically varying the number of major factors and the proportion of variance accounted for by model approximation error, I also systematically varied the number of factor indicators (i.e., items loading on each factor) , $\textrm{Items/Factor} \in \{5, 10 \}$, and the number of subjects per item, $\textrm{Subjects/Item} \in \{ 5, 10, 15\}$. The total numbers of items and sample sizes for each factor number condition can be found in Table \@ref(tab:items-subjects-table). Each item loaded on only one factor and item factor loadings were uniformly fixed at one of three levels, $\textrm{Loading} \in \{ .3, .5, .8 \}$. Though "rules-of-thumb" for factor loadings vary, Hair, Andersen, Tatham, and Black [-@hair1998, p. 111] note that "factor loadings greater than $\pm 0.3$ are considered to meet the minimal level ... if the loadings are $\pm 0.5$ or greater, they are considered practically significant." Factor loadings of $\pm 0.8$ are considered to be high [e.g., @maccallum2001]. Thus, the three factor loadings investigated in this study were chosen to represent low, moderate, and high levels of factor salience.

(ref:items-subjects-table-caption) Number of items and subjects resulting from each combination of number of factors (Factors), number of items per factor (Items/Factor), and subjects per item (Subjects/Item).

```{r items-subjects-table, echo = FALSE, results = "asis"}
# Create a table giving the total numbers of items and subjects in each factor
# number condition
x <- expand.grid("Factors" = c(1, 3, 5, 10, 15),
                 "Items/Factor" = c(5, 10),
                 "Subjects/Item" = c(5, 10, 15))
x$'Items' <- x$Factors * x$`Items/Factor`
x$'Sample Size' <- x$Items * x$`Subjects/Item`

knitr::kable(
  x = x, 
  caption = "(ref:items-subjects-table-caption)",
  digits = 0,
  longtable = TRUE,
  booktabs = TRUE
)
```

The combinations of the independent variables specified above resulted in a fully-crossed design with $4 \: (\textrm{Factors}) \times 3 \: (\textrm{Error}) \times 2 \: (\textrm{Items/Factor}) \times 3 \: (\textrm{Subjects/Item}) \times 3 \: (\textrm{Loading}) = 216$ unique conditions. For each of these conditions, I used the `simFA` function in the R `fungible` library [@R-base; @waller2019a] to generate 1,000 random sets of data in accordance with the factor model corresponding to that condition. To obtain binary responses from continuous observed scores, items were assigned classical item difficulties [$p$; i.e., the expected proportion of correct responses, @CrockerAlgina] at equal intervals between 0.15 and 0.85. For example, items in a five-item data set were assigned classical item difficulties of .150, .325, .500, .675, and .850. The classical item difficulties were used to obtain threshold values, $t$, such that $P(X > t) = p$ where $X \sim N(0,1)$. I then used the thresholds to dichotomize the continuous observed scores and obtain simulated binary response data. If a data set had any homogeneous item response vectors (i.e., had one or more items with zero variance), the data set was discarded and a new sample of data was generated until all items had non-homogeneous response vectors. This procedure was necessary to calculate tetrachoric correlation matrices in the next step of the simulation.

In the second step of the simulation procedure, I calculated a tetrachoric correlation matrix for each simulated binary data set. Tetrachoric correlation matrices were calculated using the `tetcor` function in the R `fungible` library [@waller2019a], which computes maximum likelihood tetrachoric correlation coefficients corrected for bias using the method of Brown and Benedetti [-@brown1977mean]. If a tetrachoric correlation matrix was NPD, the Higham (2002), Bentler-Yuan (2011), and Knol-Berger (1991) matrix smoothing algorithms were applied to the NPD tetrachoric correlation matrix to produce three smoothed, PSD correlation matrices.

In the third and final step of the simulation procedure, I applied three exploratory factor extraction algorithms (principal axes [PA], unweighted least squares [ULS], and maximum likelihood [ML] factor analysis) to each of the PSD and NPD tetrachoric correlation matrices and the smoothed correlation matrices. Each of the factor solutions were then rotated using a quartimin rotation and aligned to match the corresponding population factor loading matrix such that the least squares discrepancy between the matrices was minimized. The alignment step ensured that the elements of each estimated factor loading matrix were matched (in order and sign) to the elements of the corresponding population factor loading matrix.

# Results

Some text.

## Recovery of the population correlation matrix

One of the primary reasons for conducting the present simulation study was to determine which of the three smoothing methods I investigated --- the Higham (2002), Bentler-Yuan (2011), or Knol-Berger (1991) algorithms --- resulted in smoothed correlation matrices that were closest to the correlation matrix implied by major factor model (i.e., the factor model not including the minor factors). More generally, I was also interested in whether smoothed correlation matrices were closer to the model-implied correlation matrix than the unsmoothed, NPD correlation matrix. In this context, the scaled distance between two $p \times p$ correlation matrices $\mathbf{A} = \{a_{ij}\}$ and $\mathbf{B} = \{ b_{ij} \}$ was computed as:

$$
\mathrm{D}_{\mathrm{s}}(\mathbf{A}, \mathbf{B})=\sqrt{\sum_{i=1}^{p-1} \sum_{j=i+1}^{p}\left(a_{i j}-b_{i j}\right)^{2} /(p(p-1))}.
$$
```{r RpopRsm-summary, echo = FALSE}
# Convergence rates for the smoothing algorithms
smoothing_convergence_tab <- results_matrix_npd %>%
  group_by(smoothing_method) %>%
  summarise(n_obs = n(),
            n_converged = sum(!is.na(distance_Rpop_Rsm), na.rm = TRUE),
            prop_converged = mean(!is.na(distance_Rpop_Rsm), na.rm = TRUE))

# Mean D(Rpop, Rsm) by smoothing method
# For incomplete data
RpopRsm_summary_tab <- results_matrix_npd %>%
  group_by(smoothing_method) %>%
  summarise(n_obs = n(),
            RpopRsm_mean = mean(distance_Rpop_Rsm, na.rm = TRUE),
            RpopRsm_sd = sd(distance_Rpop_Rsm, na.rm = TRUE))

# For imputed data
im_data_RpopRsm <- readRDS(paste0(project_dir, "/Data", "/im_data_Rpop_Rsm_m10.RDS"))
im_data_RpopRsm <- mice::complete(im_data_RpopRsm, action = "long")
RpopRsm_summary_tab_im <- im_data_RpopRsm %>%
  group_by(smoothing_method) %>%
  summarize(n = n(),
            RpopRsm_mean = mean(exp(log_distance_Rpop_Rsm)),
            RpopRsm_sd = sd((exp(log_distance_Rpop_Rsm))))
```

To understand which of the smoothing algorithms most often produced a smoothed correlation matrix, $\Rsm$, that was closest to the model-implied correlation matrix, $\Rpop$, I calculated $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$ for each $\Rsm$. Small values of $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$ indicated that the the smoothed correlation matrix was a good approximation of $\Rpop$, whereas large values indicated that $\Rsm$ was a poor approximation of $\Rpop$. Unfortunately, the Bentler-Yuan algorithm did not converge in many cases (`r (1  - smoothing_convergence_tab$prop_converged[smoothing_convergence_tab$smoothing_method == "BY"]) * 100`%) and the Higham algorithm failed to converge in a much smaller number of cases ($<0.01\%$). After excluding these observations, I calculated the mean $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$ for each of the 1,431,279 smoothed matrices remaining. On average, the Bentler-Yuan algorithm produced smoothed correlation matrices that were slightly closer to $\Rpop$ (*M* = `r RpopRsm_summary_tab$RpopRsm_mean[RpopRsm_summary_tab$smoothing_method == "BY"]`, *SD* = `r RpopRsm_summary_tab$RpopRsm_sd[RpopRsm_summary_tab$smoothing_method == "BY"]`) than the smoothed matrices produced by the Knol-Berger (*M* = `r RpopRsm_summary_tab$RpopRsm_mean[RpopRsm_summary_tab$smoothing_method == "KB"]`, *SD* = `r RpopRsm_summary_tab$RpopRsm_sd[RpopRsm_summary_tab$smoothing_method == "KB"]`) or Higham (*M* = `r RpopRsm_summary_tab$RpopRsm_mean[RpopRsm_summary_tab$smoothing_method == "APA"]`, *SD* = `r RpopRsm_summary_tab$RpopRsm_sd[RpopRsm_summary_tab$smoothing_method == "APA"]`) algorithms. The mean distance between the NPD correlation matrix, $\Rnpd$, and $\Rpop$ was larger than the mean distances for any of the three smoothing algorithms (*M* = `r RpopRsm_summary_tab$RpopRsm_mean[RpopRsm_summary_tab$smoothing_method == "None"]`, *SD* = `r RpopRsm_summary_tab$RpopRsm_sd[RpopRsm_summary_tab$smoothing_method == "None"]`). Because these means were potentially biased by the exclusion of the cases where the Bentler-Yuan and Higham algorithms did not converge, also calculated the mean distances (by smoothing method) for each of 25 imputed data sets. The means of the mean distances for each smoothing method over ten imputed data sets differed only slightly ($\pm0.001$) from those calculated from the complete-case data. These values are reported in Appendix **X** along with a description of the imputation procedure.

To get a more detailed look at the how the smoothed correlation matrices produced by each smoothing algorithm approximated $\Rpop$, I fit a linear mixed-effects model regressing $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$ on number of subjects per item (5, 10, 15), number of items per factor (5, 10), number of factors (1, 3, 5, 10), factor loading, model error, smoothing algorithm (Higham, Bentler-Yuan, Knol-Berger, or no smoothing), all two-way interactions between these variables, and a random intercept estimated for every unique NPD correlation matrix.[^2] The estimated fixed-effect coefficients for the model fit on the non-imputed data are shown in blue in Figure \@ref(fig:coefplot-RpopRsm). Pooled fixed-effect coefficient estimates for the ten imputed data sets are shown in orange in the same figure. A full summary table for the models appears in Table \@ref(tab:distance-mod-summary).

[^2]: All numeric predictors were scaled to have a mean of zero and variance of one prior to analysis. Diagnostic plots can be found in Appendix **X**.

Looking at Figure \@ref(fig:coefplot-RpopRsm), it is clear that only a few variables had non-trivial effects on population matrix recovery. In particular, the three most potent effects were number of factors, number of subjects per item, and number of items per factor. These estimated effects were all negative, indicating better recovery of the population correlation matrix for models with larger numbers of major factors, larger sample sizes, and larger numbers of items, all else being equal. Smaller  estimated (negative) effects were seen for the use of the Bentler-Yuan smoothing algorithm (2011) and the size of the factor loadings. These results indicated that the use of the Bentler-Yuan smoothing algorithm led to slightly better recovery of the population correlation matrix, compared to the alternative smoothing methods. Additionally, higher factor loadings (i.e., increased item salience) also seemed to improve recovery of the population correlation matrix. Although high numbers of subjects per item, number of factors, and high factor loadings all seemed to lead to better recovery of the population correlation matrices, some of the benefits associated with these effects were offset by positive interaction effects. In particular, there were small, positive estimated effects for the interaction between number of subjects per item and the number of factors, the interaction between the number of factors and the size of the factor loadings, and the interaction between the size of the factor loadings and the use of the Bentler-Yuan smoothing algorithm (see Table \@ref(tab:distance-mod-summary)).

To get a better sense of how the most important variables affected population correlation matrix recovery, Figure \@ref(fig:distance-Rpop-Rsm) shows box-plots of $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$ for all combinations of smoothing method, factor loading size, number of subjects per item, and number of items per factor. The most apparent feature of Figure \@ref(fig:distance-Rpop-Rsm) was the improvement of population correlation matrix recovery as number of items per factor and number of subjects per item increase. The conditions with loadings fixed at 0.3, and 15 subjects per item (see the cells in the upper right-hand corner of Figure \@ref(fig:distance-Rpop-Rsm)) might seem go against the trend of lower $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$ for higher numbers of subjects per item. However, these aberrant results might be due to the lack of NPD correlation matrices for these combinations of conditions (only `r results_matrix_npd %>% filter(factor_loading == 0.3 & subjects_per_item == 15 & items_per_factor == 5) %>% select(id) %>% unique() %>% nrow()` and `r results_matrix_npd %>% filter(factor_loading == 0.3 & subjects_per_item == 15 & items_per_factor == 10) %>% select(id) %>% unique() %>% nrow()` NPD tetrachoric correlation matrices for the five items-per-factor and ten items-per-factor conditions, respectively). The numbers of NPD matrices represented in each cell of Figure \@ref(fig:distance-Rpop-Rsm) are provided in Appendix **X**.

```{r coefplot-RpopRsm, echo = FALSE, fig.cap = 'Coefficient estimates for the linear mixed effects model using $\\log[\\mathrm{D}_{\\mathrm{s}}(\\Rsm, \\Rpop)]$ as the dependent variable and estimating a random intercept for each NPD correlation matrix. The Bentler-Yuan (2011) and Knol-Berger (1991) algorithms are denoted as BY, and KB, respectively. The condition where no smoothing was applied is denoted as None. The effects of the Higham smoothing algorithm (2002) are subsumed within the Constant term.', out.width='100%'}
if (generate_figs) {
  source("R/figure_code/coefplots.R")
}

knitr::include_graphics(path = paste0(project_dir, 
                                      "/Text/figs/RpopRsm_coefplot.png"),
                        dpi = 320)
```

```{r distance-Rpop-Rsm, fig.cap = "Scaled distance between the smoothed ($\\Rsm$) and model-implied ($\\Rpop$) correlation matrices for the Higham (APA; 2002), Bentler-Yuan (BY; 2011), and Knol-Berger (KB; 1991) smoothing methods and when no smoothing was applied (None).", eval = TRUE, echo = FALSE, out.width='100%'}
# fig.env = "sidewaysfigure" for sideways figure, if needed
if (generate_figs) {
  # Plots of distance between Rsm and Rpop by smoothing method
  distance_Rpop_Rsm_plot <- results_matrix_npd %>%
      ggplot(aes(x = smoothing_method, y = distance_Rpop_Rsm)) +
      geom_boxplot(outlier.size = 0.5, na.rm = TRUE, outlier.alpha = 0.5) +
      facet_grid(factor_loading_rec ~ subjects_per_item_rec * items_per_factor_rec) +
      labs(x = "Smoothing method",
           y = TeX("$D_s(\\mathbf{R}_{Sm}, \\mathbf{R}_{Pop})$")) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Save png; retina gives 320 dpi
  ggsave(filename = "distance_Rpop_Rsm.png",
         plot = distance_Rpop_Rsm_plot,
         path = "figs",
         width = 7,
         height = 4.25,
         units = "in",
         device = png(),
         dpi = "retina")
}

knitr::include_graphics(path = paste0(project_dir, 
                                      "/Text/figs/distance_Rpop_Rsm.png"),
                        dpi = 320)
```

```{r distance-mod-summary, eval = TRUE, results = 'asis', echo = FALSE}
# extract() method; allows texreg() to get coefficient estimates and standard
# errors from multiple imputed lmer models.
extract.mira <- function(model) {
  gof           <- numeric()
  gof.names     <- character()
  gof.decimal   <- logical()
  
  model_ex      <- model$analyses[[1]]
  s             <- summary(pool(model))
  s_ex          <- summary(model$analyses[[1]])
  coef_names    <- rownames(s)
  betas         <- s$estimate
  se            <- s$std.error
  pval          <- s$p.value
  
  n           <- dim(model.frame(model_ex))[1]
  gof         <- c(gof, n)
  gof.names   <- c(gof.names, "Num.\ obs.")
  gof.decimal <- c(gof.decimal, FALSE)
  
  grps        <- s_ex$ngrps
  grp.names   <- names(grps)
  grp.names   <- paste("Num.\ groups:", grp.names)
  gof         <- c(gof, grps)
  gof.names   <- c(gof.names, grp.names)
  gof.decimal <- c(gof.decimal, rep(FALSE, length(grps)))
  
  tr <- createTexreg(
    coef.names = coef_names,
    coef = betas,
    se = se,
    pvalues = pval,
    gof.names = gof.names,
    gof = gof,
    gof.decimal = gof.decimal
  )
  
  return(tr)
}

if (generate_figs) {
  RpopRsm_mod <- readRDS(file = paste0(project_dir, "/Data", "/RpopRsm_model.RDS"))
  
  coef_names <- c(
    "(Intercept)" = "Constant",
    "scale(subjects_per_item)" = "Subjects/Item",
    "scale(items_per_factor)" = "Items/Factor",
    "scale(factors)" = "Factors",
    "scale(factor_loading)" = "Factor Loading",
    "scale(model_error)" = "Model Error",
    "smoothing_methodBY" = "Smoothing Method (BY)",
    "smoothing_methodKB" = "Smoothing Method (KB)",
    "smoothing_methodNone" = "Smoothing Method (None)",
    "scale(subjects_per_item):scale(items_per_factor)" = "Subjects/Item $\\times$ Items/Factor",
    "scale(subjects_per_item):scale(factors)" = "Subjects/Item $\\times$ Factors",
    "scale(subjects_per_item):scale(factor_loading)" = "Subjects/Item $\\times$ Factor Loading",
    "scale(subjects_per_item):scale(model_error)" = "Subjects/Item $\\times$ Model Error",
    "scale(subjects_per_item):smoothing_methodBY" = "Subjects/Item $\\times$ Smoothing Method (BY)",
    "scale(subjects_per_item):smoothing_methodKB" = "Subjects/Item $\\times$ Smoothing Method (KB)",
    "scale(subjects_per_item):smoothing_methodNone" = "Subjects/Item $\\times$ Smoothing Method (None)",
    "scale(items_per_factor):scale(factors)" = "Items/Factor $\\times$ Factors",
    "scale(items_per_factor):scale(factor_loading)" = "Items/Factor $\\times$ Factor Loading",
    "scale(items_per_factor):scale(model_error)" = "Items/Factor $\\times$ Model Error",
    "scale(items_per_factor):smoothing_methodBY" = "Items/Factor $\\times$ Smoothing Method (BY)",
    "scale(items_per_factor):smoothing_methodKB" = "Items/Factor $\\times$ Smoothing Method (KB)",
    "scale(items_per_factor):smoothing_methodNone" = "Items/Factor $\\times$ Smoothing Method (None)",
    "scale(factors):scale(factor_loading)" = "Factors $\\times$ Factor Loading",
    "scale(factors):scale(model_error)" = "Factors $\\times$ Model Error",
    "scale(factors):smoothing_methodBY" = "Factors $\\times$ Smoothing Method (BY)",
    "scale(factors):smoothing_methodKB" = "Factors $\\times$ Smoothing Method (KB)",
    "scale(factors):smoothing_methodNone" = "Factors $\\times$ Smoothing Method (None)",
    "scale(factor_loading):scale(model_error)" = "Factor Loading $\\times$ Model Error",
    "scale(factor_loading):smoothing_methodBY" = "Factor Loading $\\times$ Smoothing Method (BY)",
    "scale(factor_loading):smoothing_methodKB" = "Factor Loading $\\times$ Smoothing Method (KB)",
    "scale(factor_loading):smoothing_methodNone" = "Factor Loading $\\times$ Smoothing Method (None)",
    "scale(model_error):smoothing_methodBY" = "Model Error $\\times$ Smoothing Method (BY)",
    "scale(model_error):smoothing_methodKB" = "Model Error $\\times$ Smoothing Method (KB)",
    "scale(model_error):smoothing_methodNone" = "Model Error $\\times$ Smoothing Method (None)"
  )
  
  RpopRsm_coef_tab <- texreg::texreg(
    l = list(RpopRsm_mod, extract.mira(RpopRsm_imputed_model)),
    custom.coef.names = coef_names,
    custom.model.names = c("Non-imputed", "Imputed (Pooled)"),
    digits = 4,
    longtable = TRUE,
    use.packages = FALSE,
    single.row = TRUE,
    caption.above = TRUE,
    caption = "Coefficient estimates and standard errors for the linear mixed effects model using $\\log[\\mathrm{D}_{\\mathrm{s}}(\\Rsm, \\Rpop)]$ as the dependent variable and estimating a random intercept for each NPD correlation matrix.",
    label = "tab:distance-mod-summary")
  writeLines(RpopRsm_coef_tab, 
             paste0(project_dir, "/Text", "/tabs", "/RpopRsm_coef_tab.txt"))
}

cat(readLines(paste0(project_dir, "/Text", "/tabs", "/RpopRsm_coef_tab.txt")),
    sep = "\n")
```

Taken as a whole, my results suggest that three variables that account for most of the variation in $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$: (a) the number of major factors in the data-generating model, (b) the number of subjects per item, and (c) the number of items per (major) factor. Increases in any of these variables were associated with better population correlation matrix recovery. Choice of smoothing method was also related to population correlation matrix recovery, to some extent. In particular, smoothed matrices were closer to the population correlation matrix than the unsmoothed tetrachoric correlation matrices. Of the three smoothing algorithms used, the Bentler-Yuan algorithm produced smoothed matrices that were closest to the population correlation matrices. However, differences between smoothing algorithms were small except in conditions with few subjects per item, few items per factor, and low factor loadings.

## Recovery of factor loadings

After analyzing the results of my simulations in terms of $\mathrm{D}_{\mathrm{s}}(\Rsm, \Rpop)$, I next analyzed the results in terms of factor loading recovery. In particular, I wanted to determine whether factor analysis of smoothed matrices led to better factor loading estimates than unsmoothed matrices and if some smoothing methods led to better factor loading estimates than others. I was also interested in whether the interactions between smoothing methods and the other variables included in my simulation (e.g., number of items per factor, number of subjects per item, factor extraction method, etc.) would affect relative smoothing algorithm performance in terms of factor loading estimation. For the purposes of these analyses, I evaluated factor loading recovery using the root-mean-square error (RMSE) between the estimated and population factor loadings for the major factors. Given a matrix of estimated factor loadings $\hat{\mathbf{\Lambda}} = \{ \hat{\lambda}_{ij} \}_{p \times m}$, and the corresponding matrix of population factor loadings, $\mathbf{\Lambda} = \{ \lambda_{ij} \}_{p \times m}$,

$$
\textrm{RMSE}(\mathbf{\Lambda}, \hat{\mathbf{\Lambda}}) = \sqrt{\frac{\sum_{i = 1}^{p} \sum_{j = 1}^{m} (\lambda_{ij} - \hat{\lambda}_{ij})^2}{pm}}.
$$

```{r calculate-convergence-and-rmse, echo = FALSE}
# Convergence rates for the factor extraction algorithms
fa_convergence_tab <- results_matrix_npd %>%
  filter(!is.na(distance_Rpop_Rsm)) %>%
  group_by(fa_method_rec) %>%
  summarise(n_obs = n(),
            n_converged = sum(fa_convergence, na.rm = TRUE),
            prop_converged = mean(fa_convergence, na.rm = TRUE))

# Mean RMSE values by smoothing method and factor extraction method
# For incomplete data
loading_summary_tab <- results_matrix_npd %>%
  filter(!is.na(distance_Rpop_Rsm)) %>%
  group_by(smoothing_method) %>%
  summarise(n_obs = n(),
            mean_rmse = mean(loading_rmsd, na.rm = TRUE),
            sd_rmse = sd(loading_rmsd, na.rm = TRUE))

# For imputed data
im_data <- readRDS(paste0(project_dir, "/Data", "/im_data_m10.RDS"))
im_data <- mice::complete(im_data, action = "long")
loading_im_summary_tab <- im_data %>%
  group_by(smoothing_method) %>%
  summarize(n = n(),
            mean_rmse = mean(exp(log_loading_rmsd)),
            sd_rmse = sd((exp(log_loading_rmsd))))

# Save the number of PA non-converged matrices and number of remaining cases as
# objects that can be easily referenced in the text.
pa_nonconverged <- fa_convergence_tab$n_obs[fa_convergence_tab$fa_method_rec == "Principal Axes"] -
  fa_convergence_tab$n_converged[fa_convergence_tab$fa_method_rec == "Principal Axes"]
pa_convergence_rate <- fa_convergence_tab$prop_converged[fa_convergence_tab$fa_method_rec == "Principal Axes"]
n_complete_obs <- results_matrix_npd %>%
  filter(!is.na(loading_rmsd) & fa_convergence != FALSE) %>%
  nrow()
```

To determine which smoothing method resulted in the best factor loading estimates, I calculated the $\RMSE$ for each pair of estimated and population factor loading matrices. Small $\RMSE$ values indicated that the estimated factor loading matrices were quite similar to their corresponding population factor loading matrices, whereas large $\RMSE$ values indicated poorly-estimated factor loading matrices. As in the previous section, cases where the Higham (2002) or Bentler-Yuan (2011) algorithms did not converge were not included in my analyses. Furthermore, cases where the principal axes factor extraction algorithm failed to converge were also not included in my analyses. In total, there were only `r prettyNum(pa_nonconverged, big.mark = ",")` cases where the PA algorithm did not converge (convergence rate = `r pa_convergence_rate`) and only four cases where the ML algorithm did not converge (convergence rate > 0.999). For the `r prettyNum(n_complete_obs, big.mark = ",")` cases remaining, factor analysis of the Bentler-Yuan (2011) smoothed matrices resulted in the lowest mean $\RMSE$ (*M* = `r loading_summary_tab$mean_rmse[loading_summary_tab$smoothing_method == "BY"]`, *SD* = `r loading_summary_tab$sd_rmse[loading_summary_tab$smoothing_method == "BY"]`) whereas the smoothed matrices produced by the Higham (2002; *M* = `r loading_summary_tab$mean_rmse[loading_summary_tab$smoothing_method == "APA"]`, *SD* = `r loading_summary_tab$sd_rmse[loading_summary_tab$smoothing_method == "APA"]`) and Knol-Berger (1991; *M* = `r loading_summary_tab$mean_rmse[loading_summary_tab$smoothing_method == "KB"]`, *SD* = `r loading_summary_tab$sd_rmse[loading_summary_tab$smoothing_method == "KB"]`) algorithms led to slightly higher mean $\RMSE$ values. Factor analyzing unsmoothed NPD tetrachoric correlation matrices led to the highest mean $\RMSE$ (*M* = `r loading_summary_tab$mean_rmse[loading_summary_tab$smoothing_method == "None"]`, *SD* = `r loading_summary_tab$sd_rmse[loading_summary_tab$smoothing_method == "None"]`). The mean $\RMSE$ values for each of the smoothing methods from ten sets of imputed data were less distinct; the Bentler-Yuan algorithm (2011) gave the best results (*M* = `r loading_im_summary_tab$mean_rmse[loading_im_summary_tab$smoothing_method == "BY"]`, *SD* = `r loading_summary_tab$sd_rmse[loading_summary_tab$smoothing_method == "BY"]`), followed by the Knol-Berger algorithm (1991; *M* = `r loading_im_summary_tab$mean_rmse[loading_im_summary_tab$smoothing_method == "KB"]`, *SD* = `r loading_summary_tab$sd_rmse[loading_summary_tab$smoothing_method == "KB"]`), Higham (2002; *M* = `r loading_im_summary_tab$mean_rmse[loading_im_summary_tab$smoothing_method == "APA"]`, *SD* = `r loading_summary_tab$sd_rmse[loading_summary_tab$smoothing_method == "APA"]`) and no smoothing methods (*M* = `r loading_im_summary_tab$mean_rmse[loading_im_summary_tab$smoothing_method == "None"]`, *SD* = `r loading_summary_tab$sd_rmse[loading_summary_tab$smoothing_method == "None"]`).

To obtain estimates of the effects of the variables in my study on $\RMSE$, I fit a linear mixed-effects model regressing $\RMSE$ on number of subjects per item (5, 10, 15), number of items per factor (5, 10), number of factors (1, 3, 5, 10), factor loading, model error, smoothing algorithm (Higham, Bentler-Yuan, Knol-Berger, or no smoothing), factor extraction method (PA, ULS, or ML), all two-way interactions between these variables, and a random intercept estimated for every unique NPD correlation matrix.[^3] The estimated fixed-effect coefficients for the model fit on the non-imputed data are shown in blue in Figure \@ref(fig:coefplot-loading-recovery). Pooled estimates of the fixed-effect coefficients from the ten imputed data sets are shown in orange in the same figure. A full summary table including (untransformed) coefficient estimates and standard errors for the model fit with the non-imputed data appears in Table \@ref(tab:loading-mod-summary). **WOULD BE NICE TO HAVE COEF ESTIMATES FROM BOTH MODELS**.

[^3]: All numeric predictors were scaled to have a mean of zero and variance of one prior to analysis. Diagnostic plots can be found in Appendix **X**.

Looking at Figure \@ref(fig:coefplot-loading-recovery), one can see that only a few variables had non-negligible effects on factor loading recovery. None of the effects of primary interest to this study---the main effects or two-way interactions involving the smoothing methods---were large enough to hold much practical significance. For instance, the results indicated that not applying smoothing to an NPD tetrachoric correlation matrix prior to factor extraction led to the worst factor loading estimates. Although this effect was statistically significant, it corresponds to only a minute improvement in $\RMSE$ for smoothed compared to unsmoothed NPD tetrachoric correlation matrices (see Figure \@ref(fig:coefplot-loading-recovery) and Table \@ref(tab:loading-mod-summary)).

Although none of the primary effects of interest to this study were large, there were some estimated effects that, although ancillary for this study, were large enough to warrant mention. In particular, there were moderate, positive effects for maximum likelihood factor extraction and the interactions between ML and factor loading, ML and subjects per item, and ML and items per factor. These results suggest that maximum likelihood factor extraction led to worse factor loading recovery than the alternatives (PA or ULS). Moreover, factor loading estimates for ML were not improved as much by increasing factor loadings, subjects per item, or items per factor (which were associated with large, negative effects) compared to PA or ULS. The effects of factor extraction method, factor loading, and number of items are shown in Figure \@ref(fig:fa-method-boxplots), which contains box plots of $\RMSE$ for each combination of factor extraction method, factor loading, and number of subjects per item. Looking at each cell of Figure \@ref(fig:fa-method-boxplots), notice that ML often led to the lowest $\RMSE$ value in conditions with small factor loadings but did not improve as much as ULS or PA as factor loadings increased. Similar effects can be seen for the number of subjects per item; although $\RMSE$ values generally decreased as number of subjects per item increased for all factor extraction methods, ML seemed to benefit least.[^4]

[4^] Number of items per factor was not included in Figure \@ref(fig:fa-method-boxplots) because the differences in $\RMSE$ across levels of the condition were too small to be clearly seen.

```{r fa-method-boxplots, fig.cap="Box plots of $\\RMSE$ for all combinations of factor extraction method, factor loading, and number of subjects per item. The three factor extraction methods (unweighted least squares, maximum likelihood, and principal axes) are denoted by ULS, ML, and PA, respectively."}
if (generate_figs) {
  rmse_fa_method <- results_matrix_npd %>%
  filter(fa_convergence == TRUE) %>%
  ggplot(aes(x = fa_method_rec, 
             y = loading_rmsd)) +
  geom_boxplot(outlier.size = 0.5, na.rm = TRUE, outlier.alpha = 0.5) +
  facet_grid(subjects_per_item_rec ~ factor_loading_rec) +
  labs(x = "Extraction Method",
       y = TeX("$RMSE(\\mathbf{\\Lambda}, \\hat{\\mathbf{\\Lambda}})$")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Save png; retina gives 320 dpi
  ggsave(filename = "fa_method_boxplots.png",
         plot = rmse_fa_method,
         path = "figs",
         width = 5,
         height = 7,
         units = "in",
         device = png(),
         dpi = "retina")
}

knitr::include_graphics(path = paste0(project_dir, 
                                      "/Text/figs/fa_method_boxplots.png"),
                        dpi = 320)
```

In addition to the effects that have already been discussed, there was also a relatively large and seemly anomalous negative effect for number of factors. On its face, this seems to suggest that data generated from models with large numbers of major factors led to better factor loading recovery. However, this effect is most likely due to the fact that, whereas number of items per factor and number of subjects per item are orthogonal to number of factors, the total sample size and total number of items for each data set were confounded with number of factors. I.e., conditions with larger numbers of factors tended to have more items total and therefore also tended to have larger sample sizes despite having the same numbers of indicators (items) per factor and numbers of subjects per indicator. The strong relationship between $\log \RMSE$ and total sample size can be clearly seen in Figure **X**, which shows that $\log \RMSE$ decreased as sample size increased. Therefore, it seems reasonable to conclude that the effect of number of factors can be better understood as being related to the total number of items and subjects in a data set.

```{r RMSE-sample-size, fig.cap="Log root-mean-square error (RMSE) between the true and estimated factor loading matrices as a function of sample size. Due to the large number of data points, hexagonal bins were used to group observations with the density of each hexagon represented by its color."}
if (generate_figs) {
  rmse_sample_size <- results_matrix_npd %>%
  filter(fa_convergence == TRUE) %>%
  ggplot(aes(x = as.numeric(items_per_factor * factors * subjects_per_item), 
             y = log(loading_rmsd))) +
  geom_hex() +
  labs(x = "Sample size",
       y = TeX("$\\log \\; RMSE(\\mathbf{\\Lambda}, \\hat{\\mathbf{\\Lambda}})$")) +
  theme_minimal()
  
  # Save png; retina gives 320 dpi
  ggsave(filename = "rmse_sample_size.png",
         plot = rmse_sample_size,
         path = "figs",
         width = 6.5,
         height = 5,
         units = "in",
         device = png(),
         dpi = "retina")
}

knitr::include_graphics(path = paste0(project_dir, 
                                      "/Text/figs/rmse_sample_size.png"),
                        dpi = 320)
```

```{r coefplot-loading-recovery, eval = TRUE, echo = FALSE, fig.cap = "Coefficient estimates for the linear mixed effects model using $\\log[\\RMSE]$ as the dependent variable and estimating a random intercept for each NPD correlation matrix. The Bentler-Yuan (2011) and Knol-Berger (1991) algorithms are denoted as BY, and KB, respectively. The condition where no smoothing was applied is denoted as None. Maximum likelihood factor extraction is denoted by ML and principal axis factor extraction is denoted by PA. The effects of the Higham smoothing algorithm (2002) and unweighted least squares factor extraction are subsumed within the Constant term.", out.width='100%'}
knitr::include_graphics(path = paste0(project_dir, 
                                      "/Text/figs/loadings_coefplot.png"),
                        dpi = 320)
```

```{r loading-mod-summary, eval = TRUE, results = 'asis', echo = FALSE}
if (generate_tabs) {
  loading_mod <- readRDS(paste0(project_dir, "/Data", "/loading_model.RDS"))
  loading_imputed_mod <- readRDS(paste0(project_dir, "/Data", "/loading_imputed_model.RDS"))
  
  coef_names <- c(
    "(Intercept)" = "Constant",
    "scale(subjects_per_item)" = "Subjects/Item",
    "scale(items_per_factor)" = "Items/Factor",
    "scale(factors)" = "Factors",
    "scale(factor_loading)" = "Factor Loading",
    "scale(model_error)" = "Model Error",
    "smoothing_methodBY" = "Smoothing Method (BY)",
    "smoothing_methodKB" = "Smoothing Method (KB)",
    "smoothing_methodNone" = "Smoothing Method (None)",
    "fa_methodfaml" = "Factor Extraction (ML)",
    "fa_methodfapa" = "Factor Extraction (PA)",
    "scale(subjects_per_item):scale(items_per_factor)" = "Subjects/Item $\\times$ Items/Factor",
    "scale(subjects_per_item):scale(factors)" = "Subjects/Item $\\times$ Factors",
    "scale(subjects_per_item):scale(factor_loading)" = "Subjects/Item $\\times$ Factor Loading",
    "scale(subjects_per_item):scale(model_error)" = "Subjects/Item $\\times$ Model Error",
    "scale(subjects_per_item):smoothing_methodBY" = "Subjects/Item $\\times$ Smoothing Method (BY)",
    "scale(subjects_per_item):smoothing_methodKB" = "Subjects/Item $\\times$ Smoothing Method (KB)",
    "scale(subjects_per_item):smoothing_methodNone" = "Subjects/Item $\\times$ Smoothing Method (None)",
    "scale(subjects_per_item):fa_methodfaml" = "Subjects/Item $\\times$ Factor Extraction (ML)",
    "scale(subjects_per_item):fa_methodfapa" = "Subjects/Item $\\times$ Factor Extraction (PA)",
    "scale(items_per_factor):scale(factors)" = "Items/Factor $\\times$ Factors",
    "scale(items_per_factor):scale(factor_loading)" = "Items/Factor $\\times$ Factor Loading",
    "scale(items_per_factor):scale(model_error)" = "Items/Factor $\\times$ Model Error",
    "scale(items_per_factor):smoothing_methodBY" = "Items/Factor $\\times$ Smoothing Method (BY)",
    "scale(items_per_factor):smoothing_methodKB" = "Items/Factor $\\times$ Smoothing Method (KB)",
    "scale(items_per_factor):smoothing_methodNone" = "Items/Factor $\\times$ Smoothing Method (None)",
    "scale(items_per_factor):fa_methodfaml" = "Items/Factor $\\times$ Factor Extraction (ML)",
    "scale(items_per_factor):fa_methodfapa" = "Items/Factor $\\times$ Factor Extraction (PA)",
    "scale(factors):scale(factor_loading)" = "Factors $\\times$ Factor Loading",
    "scale(factors):scale(model_error)" = "Factors $\\times$ Model Error",
    "scale(factors):smoothing_methodBY" = "Factors $\\times$ Smoothing Method (BY)",
    "scale(factors):smoothing_methodKB" = "Factors $\\times$ Smoothing Method (KB)",
    "scale(factors):smoothing_methodNone" = "Factors $\\times$ Smoothing Method (None)",
    "scale(factors):fa_methodfaml" = "Factors $\\times$ Factor Extraction (ML)",
    "scale(factors):fa_methodfapa" = "Factors $\\times$ Factor Extraction (PA)",
    "scale(factor_loading):scale(model_error)" = "Factor Loading $\\times$ Model Error",
    "scale(factor_loading):smoothing_methodBY" = "Factor Loading $\\times$ Smoothing Method (BY)",
    "scale(factor_loading):smoothing_methodKB" = "Factor Loading $\\times$ Smoothing Method (KB)",
    "scale(factor_loading):smoothing_methodNone" = "Factor Loading $\\times$ Smoothing Method (None)",
    "scale(factor_loading):fa_methodfaml" = "Factor Loading $\\times$ Factor Extraction (ML)",
    "scale(factor_loading):fa_methodfapa" = "Factor Loading $\\times$ Factor Extraction (PA)",
    "scale(model_error):smoothing_methodBY" = "Model Error $\\times$ Smoothing Method (BY)",
    "scale(model_error):smoothing_methodKB" = "Model Error $\\times$ Smoothing Method (KB)",
    "scale(model_error):smoothing_methodNone" = "Model Error $\\times$ Smoothing Method (None)",
    "scale(model_error):fa_methodfaml" = "Model Error $\\times$ Factor Extraction (ML)",
    "scale(model_error):fa_methodfapa" = "Model Error $\\times$ Factor Extraction (PA)",
    "smoothing_methodBY:fa_methodfaml" = "Smoothing Method (BY) $\\times$ Factor Extraction (ML)",
    "smoothing_methodKB:fa_methodfaml" = "Smoothing Method (KB) $\\times$ Factor Extraction (ML)",
    "smoothing_methodNone:fa_methodfaml" = "Smoothing Method (None) $\\times$ Factor Extraction (ML)",
    "smoothing_methodBY:fa_methodfapa" = "Smoothing Method (BY) $\\times$ Factor Extraction (PA)",
    "smoothing_methodKB:fa_methodfapa" = "Smoothing Method (KB) $\\times$ Factor Extraction (PA)",
    "smoothing_methodNone:fa_methodfapa" = "Smoothing Method (None) $\\times$ Factor Extraction (PA)"
  )
  
  loading_coef_tab <- texreg::texreg(
    l = list(loading_mod, extract.mira(loading_imputed_model)),
    custom.coef.names = coef_names,
    custom.model.names = c("Non-imputed", "Imputed (Pooled)"),
    digits = 4,
    single.row = TRUE,
    longtable = TRUE,
    use.packages = FALSE,
    caption.above = TRUE,
    caption = "Coefficient estimates and standard errors for the linear mixed effects model using $\\log[\\RMSE]$ as the dependent variable and estimating a random intercept for each NPD correlation matrix.",
    label = "tab:loading-mod-summary"
  )
  
  writeLines(loading_coef_tab, paste0(project_dir, "/Text", "/tabs", "/loading_coef_tab.txt"))
}

cat(readLines(paste0(project_dir, "/Text", "/tabs", "/loading_coef_tab.txt")),
    sep = '\n')
```

In summary, the results of my simulation study indicated that there was no meaningful advantage of using any smoothing algorithm over any other. Moreover, there seemed to be no large advantage (in terms of $\RMSE$) to smoothing NPD tetrachoric correlation matrices prior to conducting EFA. However, the application of some matrix smoothing algorithm to and NPD tetrachoric correlation matrix still seems to be advisable in practice. All three matrix smoothing algorithms included in this study are widely-available, computationally inexpensive, and are unlikely to result in worse factor loading recovery than if the unsmoothed NPD correlation matrix is used for factor analysis.

# Discussion

\newpage

# References

```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
